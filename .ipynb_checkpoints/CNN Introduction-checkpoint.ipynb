{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "549d94c9",
   "metadata": {},
   "source": [
    "# Convolution Networks Intro Exercise\n",
    "\n",
    "In this exercise you'll learn the basics of convolution network (CNN), its benefits over regular feedforward networks (FNN) and how to implement it with `keras`.\n",
    "\n",
    "This exercise is composed of the following parts: \n",
    "1. Reminding the problem of prognostic health monitoring (PHM) by vibration signals\n",
    "2. Solving the problem using regular FNN\n",
    "3. Solving the problem using CNN\n",
    "\n",
    "*Created by Or Zamir*\n",
    "<br>*Inspected and directed by Omri Matania*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15018dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting HTML styling was successful.\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "import modules.css_settings as css\n",
    "HTML(css.get_settings())\n",
    "print(\"Setting HTML styling was successful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfc2863f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing libraries and dataset was successful.\n"
     ]
    }
   ],
   "source": [
    "## Imports\n",
    "import modules.sample_generator as generator\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras import backend\n",
    "from keras import callbacks\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score\n",
    "from scipy.stats import kurtosis\n",
    "import tensorflow as tf\n",
    "\n",
    "## Fetching Dataset\n",
    "x_train, y_train = generator.get_dataset()\n",
    "sample_size = 1024\n",
    "\n",
    "print(\"Importing libraries and dataset was successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f2c795",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "In this part is a reminder of the problem from the field of prognostic health monitoring (PHM) as introduced in the `Estimation of fault size` exercise.\n",
    "\n",
    "### Part 1.1 - Introducing the Problem\n",
    "A PHM product monitors the vibration of mechanical systems in order to sense a fault of their mechanical parts.\n",
    "The specific product we are dealing with is trying to sense a fault in tooth-wheel systems, as illustrated in **Figure 1**.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"img/tooth_wheel.png\" alt=\"bla\" width=300 height=300>\n",
    "  <figcaption><center><b>Figure 1</b> - Two tooth-wheel machine.</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "When the system has no flaws and the tooth-wheels have no faults, the sensor senses a pulsed signal, with a number of pulses as the number of the teeth.\n",
    "\n",
    "A system fault can be a hole in one of the teeth, as illustrated in **Figure 2**.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"img/tooth_defect.png\" alt=\"bla\" width=300 height=300>\n",
    "  <figcaption><center><b>Figure 2</b> - Two tooth-wheel machine with a fault.</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "This type of fault is called `full tooth face fault`.\n",
    "\n",
    "For the purpose of this exercise, the fault is modeled by an *additional pulse* in the sensor's output, with *width and height proportional to the fault's* width and depth (let's assume they are correlated, so the width and height of the pulse are proportional to each other).\n",
    "\n",
    "\n",
    "### Fault Signal Location\n",
    "**For this exercise, we won't assume that the fault appears in the middle of the signal. We will observe later why this assumption is significant for choosing an architecture.**\n",
    "\n",
    "`The code segment below generates a signal sensed by the sensor. Generate some signals and observer the fault signal location.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "869f991d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate signal\n",
    "signal = generator.generate() # For demonstration only, the fault size of this generator has fixed value of 0.7\n",
    "\n",
    "# Plot\n",
    "plt.plot(signal)\n",
    "plt.title(\"Signal Example\")\n",
    "plt.xlabel(\"Sample [N]\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51746e5",
   "metadata": {},
   "source": [
    "## Part 2 - Solving using FNN\n",
    "In this part we will train a regular FNN to detect the fault size.\n",
    "<br>We will use same architecture used in the `Estimation of fault size` which introduced good result when the fault signal was stationary for all signals. <br>**We will evaluate its performance for the case in which the fault signal varies its location over different recordings**. \n",
    "\n",
    "\n",
    "### Part 2.1 - Constructing the Architecture\n",
    "We'll construct a 3-layer architecture as illustrated in **Figure 3**. The first layer is the `input layer`, the second layer is the `hidden layer`, and the third layer is the `output layer`.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"img/architecture.png\" alt=\"bla\" style=\"width: 50%\">\n",
    "  <figcaption><center><b>Figure 3</b> - Illustration of the 3-layer architecture for this exercise. <br>The properties of each layer (size, type and activation) are shown above.</center></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9283ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_FNN_model():\n",
    "    # Creating a sequential model - a simple way to construct a model. For constructing more complex models \n",
    "    # with additional flexibility in design, one can use a Functional model.\n",
    "    model = Sequential()\n",
    "\n",
    "    # Adding the input layer with a dimension of the sample_size (1024)\n",
    "    # The input layer doesnt have any more properties.\n",
    "    model.add(keras.layers.InputLayer(input_shape=(sample_size,)))\n",
    "\n",
    "    # Adding a hidden layer with a dimension of the 64 (an arbitrary value)\n",
    "    # The type of layer is Dense, its activation is ReLu and it's weights initialized with normal distribution.\n",
    "    model.add(Dense(64, kernel_initializer='normal', activation='relu', name='hidden_layer'))\n",
    "    \n",
    "    # Adding the output later with the dimension of the output (a scalar)\n",
    "    # The type of layer is Dense, its activation is sigmoid and it's weights initialized with normal distribution.\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid', name='output_layer'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5614c90",
   "metadata": {},
   "source": [
    "### Part 2.2 - Testing Model for Varying Number of Samples\n",
    "Lets train this model for varying number of samples.\n",
    "\n",
    "*Note: this run should take about 2 minutes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c20fdde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss is: 8.38e-03\n",
      "Evaluation Loss is: 1.90e-02\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAA8eklEQVR4nO3dd3xUVfr48c+THtJJQgsldAKhF1GkiIKu0kTXtiqurmVd3a/bXF23u66u67q7/myLa1kb6iIiYkNYERtFsNB7CzUJpJI+5/fHuYEhBtJmcieT5/165ZWZe2fufc5MMs+ccs8RYwxKKaXU6YS4HYBSSqnAp8lCKaVUnTRZKKWUqpMmC6WUUnXSZKGUUqpOmiyUUkrVSZOFqjcRSRcRIyJh9XjsdSLySXPEFexEZIKIZLl4/otFZK+IFInIULfiqIuIPCcif3I7jmClySJIicguESkXkZQa279yPvDTXQqtQUknEInI7534v+u1Lczt19WPHgJuM8bEGmO+rLlTRKY7f1cFIpIjIkuC9HVo1TRZBLedwJXVd0RkIBDtXjhB5QjwRxEJdTuQhmhkgu4GrD/F8XoBzwM/AxKA7sDjgKexMarApMkiuL0AXOt1fxb2H/s4EUkQkedFJFtEdovIr0UkxNkXKiIPOd8WdwAX1fLcp0XkgIjsE5E/NfXDU0Q6icgCETkiIttE5EavfaNE5AvnG+whEXnY2R4lIi+KSK6I5InIKhFpX8ux7xKRuTW2/VNEHnFuXyciO0SkUER2isj3ThPqe0A5cPUpyrFURH7gdf+kZjmnFnKriGx1zneviPQUkc+d8r0mIhE1jvkr573Y5R2biEQ679Me53V5UkSinX0TRCRLRH4pIgeBZ2uJNcR533eLyGHn7yHBOW4REAp8LSLbaynqEGCnMWaJsQqNMa8bY/Y4xx7llCnP+Tt51LtcDXkdvMpS6+tQS7mmODWePBH5TEQGee37pfM3Wygim0Xk3FMdRzmMMfoThD/ALuA8YDOQgf2H34v9lmiAdOdxzwNvAnFAOrAFuMHZdwuwCegCtAU+dJ4b5uyfD/wLiAHaASuBm5191wGfnCK2dO/j1Nj3EfabaRT2gygbONfZ9zlwjXM7Fhjt3L4ZeAto45RzOBBfy7G7Aceq9zmPPQCMdspQAPR19nUEBpwi/t8DLwLTgB1AOBBW43VdCvzA6zknvR7OYxcA8cAAoAxYAvTAfkPfAMxyHjsBqAQeBiKB8UCxV6z/cI7V1nkf3wLur/HcvzjPja6lPNcD25xzxwLzgBdqxNrrFK9FD6AU+DtwDhBbY/9w5/UNc973jcAdfnodngP+5NweBhwGznDe51nY/4lIoC/2f6GT199jT7f/ZwP9R2sWwa+6djEJ+8G/r3qHUwu4HLjb2G+Eu4C/Adc4D7kM+IcxZq8x5ghwv9dz2wPfwf7jFxtjDmM/MK5obKAi0gU4G/ilMabUGPMV8G+veCqAXiKSYowpMsYs99qejP1AqzLGrDbGFNQ8vjFmN7AGmOFsmggc8zqOB8gUkWhjzAFjTK1NL17HW4BNZj843eNO4y/GmALnPOuARcaYHcaYfOBdoGZn8m+MMWXGmI+At4HLRESAG4GfGGOOGGMKgT9z8vvgAX7nPLeklji+BzzsnLsIuBu4oj5NVsaYHdgP8TTgNSBHbEdzrLN/tTFmuTGm0vn7+hf2Q96nr0Mtod0I/MsYs8L5m/gPNhGNBqqwSaO/iIQbY3YZY2qrNSkvmiyC3wvAVdhvts/X2JcCRAC7vbbtxv7jA3TCfgPz3letG/Yb9QGnmp+H/SBo14RYOwHVH3i1xXMD0AfY5DQ1TXG2vwC8D7wiIvtF5EERCT/FOV7mRD/OVc59jDHF2MR5i1Omt0WkXz1i/jVwD7Ym1FCHvG6X1HI/1uv+USfGaruxr1cqtka12ut9eM/ZXi3bGFN6mjg68e2/gTDgW015tXGSwWXGmFRgLDAO+5ogIn1EZKGIHBSRAmwiS6lxCF+8DjV1A35W/Zo4r0sXbG1iG3AHtoZ4WEReEZHajqG8aLIIcs636Z3AhdjmBW852G/l3by2deVE7eMA9h/Me1+1vdhvainGmETnJ94YM6AJ4e4H2opIXG3xGGO2GmOuxCakvwBzRSTGGFNhjPmDMaY/cBYwhZP7arz9F5ggIp2Bi3GShXP8940xk7BNUJuAp+oK2BjzAbYJ59Yau4qxH+LVOtR1rDokiUiM1/2u2NcrB/uBOsDrfUgwxnh/wNY1tfR+vv03UMnJH9r1YoxZhf07y3Q2PYF9LXsbY+KBXwHS0ON6OdXrUNNe4D6v1yTRGNPGGDPHifNlY8zZnGiW/UsTYmoVNFm0DjcAE2t8I8MYU4VtOrhPROJEpBvwU2x7PM6+H4tIZxFJAu7yeu4BYBHwNxGJdzpJe4pIzSaG04kU2zkdJSJR2KTwGXC/s22QE/tLACJytYikGmM8QJ5zjCoROUdEBjrNagXYBFhV2wmNMdnY/oRnsR2zG51jtxeRac4HURlQdKpj1OIe4M4a274CZopIG7Ejhm6o57FO5w8iEiEiY7EJ8b/Oa/EU8HcRaeeUJU1Ezm/AcecAPxGR7k7z0Z+BV40xlXU9UUTOFpEbvc7dD9uXU920F4d9T4qcfT9sQFyn8q3XoZbHPAXcIiJniBUjIhc5f+d9RWSiiERi+1tKqP973WppsmgFjDHbjTFfnGL37dhvwTuAT7DftJ9x9j2Fbd75GtvWX7Nmci22GWsDcBSYi/1WXl9F2H/U6p+J2CaidOy3xTewbe0fOI+/AFjvjND5J3CF07zSwTl3AbYD9SNOJLzavIzt/H/Za1sIdvjnfuyw2PF8u7ZQK2PMp9jOfW9/x46WOgT8ByfhNcFB7Gu83znWLcaYTc6+X2JrN8udpp7F2E7c+noG25S3DFsLLcX+XdRHHjY5rHXel/ew79uDzv6fY5v7CrF/T682IK7anO51OM75e78ReNR5/DZsUyzY/ooHsLWyg9ia6q+aGFfQE2N08SOlVOATkQnAi8aYzi6H0ippzUIppVSdNFkopZSqkzZDKaWUqpPWLJRSStWpRc76WZeUlBSTnp7udhhKKdVirF69Ose5sLJWAZ8sRKQHdhx7gjHm0vo8Jz09nS++ONVIUaWUUjWJyO7T7fdrM5SIPCN2Fst1NbZf4Mz0uE1E7jrV88HOPWOM8cUFTUoppRrJ3zWL57AXxRyfk8i5yvYx7MR2WcAqEVmAnRny/hrPv96ZoE4ppZSL/JosjDHL5NsrZo0CtjmzVSIirwDTjTH3Yy/dbxQRuQm4CaBr1651PFoppVRDuNFnkcbJM5lmYeecr5WIJAP3AUNF5G4nqXyLMWY2MBtgxIgROh5YqRasoqKCrKwsSktPN1muaoyoqCg6d+5MePipJmaunRvJorYZJ0/54W6MycVOG62UaiWysrKIi4sjPT0du2SH8gVjDLm5uWRlZdG9e/cGPdeN6yyyOHna687UPsWwUqqVKi0tJTk5WROFj4kIycnJjaqxuZEsVgG9nemQI7Arei1wIQ6lVADTROEfjX1d/T10dg523eS+Yhdav8GZI/827NTXG4HX6lq+stmsfAp2LHU7CqWUCjh+TRbGmCuNMR2NMeHGmM7GmKed7e8YY/oYY3oaY+7zZwz1VlkOq5+DFy+BNS+4HY1SykV5eXk8/vjjDX7ehRdeSF5enu8DCgA6N1S1sAj4/juQPhYW3AZL/ggej9tRKaVccKpkUVV1+gX13nnnHRITE/0Ulbs0WXiLSoDv/ReGzYKP/wbzfgAVOnRPqdbmrrvuYvv27QwZMoSRI0dyzjnncNVVVzFw4EAAZsyYwfDhwxkwYACzZ88+/rz09HRycnLYtWsXGRkZ3HjjjQwYMIDJkydTUlLiVnF8IuDnhmoIEZkKTO3Vq1fjDxIaDlP/Cck94YPfQn4WXPEyxKT4LE6lVP394a31bNhf4NNj9u8Uz++mDjjl/gceeIB169bx1VdfsXTpUi666CLWrVt3fLjpM888Q9u2bSkpKWHkyJFccsklJCcnn3SMrVu3MmfOHJ566ikuu+wyXn/9da6++mqflqM5BVXNwhjzljHmpoSEhKYdSATG/B989z9w4Gv493mQs9U3QSqlWpxRo0addF3CI488wuDBgxk9ejR79+5l69Zvfz50796dIUOGADB8+HB27drVTNH6R1DVLHxuwAyIT4M5V9iEccVLkH6221Ep1aqcrgbQXGJiYo7fXrp0KYsXL+bzzz+nTZs2TJgwodbrFiIjI4/fDg0NbfHNUEFVs/CLLiPhxiUQ2w6enwFfv+J2REopP4uLi6OwsLDWffn5+SQlJdGmTRs2bdrE8uXLmzk6d2jNoj6S0uGGRfDatfDGzVCwH8b+1O2olFJ+kpyczJgxY8jMzCQ6Opr27dsf33fBBRfw5JNPMmjQIPr27cvo0aNdjLT5BOUa3CNGjDB+Wfyoshzmfh+2vA+/2ArRSb4/h1KKjRs3kpGR4XYYQau211dEVhtjRpzqOdoM1RBhEXD2T8FTAZvedjsapZRqNposGiptGCR2g3Xz3I5EKaWajSaLhhKBzJl2DqniHLejUUqpZhFUyUJEporI7Pz8fP+eaMBMMFWwUSfLVUq1DkGVLHx2UV5dOgyE5N7aFKWUajWCKlk0m+qmqF2fQOFBt6NRSim/02TRWANmAgY2vOl2JEqpAFU9sWBjzJ8/nw0bNhy//9vf/pbFixf7KrQG02TRWO36QbsB2hSllPKLmsnij3/8I+edd55r8WiyaIrMi2HvcjszrVIqqLz44ouMGjWKIUOGcPPNN/PYY49x5513Ht//3HPPcfvttwOnnrK82q5du8jMzDx+/6GHHuL3v/89AE899RQjR45k8ODBXHLJJRw7dozPPvuMBQsW8Itf/IIhQ4awfft2rrvuOubOnQvAkiVLGDp0KAMHDuT666+nrKwMsDWZ3/3udwwbNoyBAweyadMmn70eOt1HUwyYCf/7E6x/A8663e1olApO794FB9f69pgdBsJ3Hjjl7o0bN/Lqq6/y6aefEh4ezq233kpsbCzz5s3jwQcfBODVV1/lnnvuAeo3ZfmpzJw5kxtvvBGAX//61zz99NPcfvvtTJs2jSlTpnDppZee9PjS0lKuu+46lixZQp8+fbj22mt54oknuOOOOwBISUlhzZo1PP744zz00EP8+9//buirUyutWTRFck/oOESbopQKMkuWLGH16tWMHDmSIUOGsGTJEnbu3EmPHj1Yvnw5ubm5bN68mTFjxgD1m7L8VNatW8fYsWMZOHAgL730EuvXrz/t4zdv3kz37t3p06cPALNmzWLZsmXH98+cORPw/bToWrNoqsyZdpGkIzugbQ+3o1Eq+JymBuAvxhhmzZrF/ffff9L2p59+mtdee41+/fpx8cUXIyL1mrI8LCwMj9cyzd77r7vuOubPn8/gwYN57rnnWLp0aZ2xnU711OihoaFUVlbWp7j1ojWLphpwsf29/g1341BK+cy5557L3LlzOXz4MABHjhxh9+7dzJw5k/nz5zNnzhwuv/xyoH5Tlrdv357Dhw+Tm5tLWVkZCxcuPL6vsLCQjh07UlFRwUsvvXR8+6mmSe/Xrx+7du1i27ZtALzwwguMHz/ep+WvjSaLpkrsCp1HwTpNFkoFi/79+/OnP/2JyZMnM2jQICZNmsSBAwdISkqif//+7N69m1GjRgF2yvLKykoGDRrEb37zm1qnLA8PD+e3v/0tZ5xxBlOmTKFfv37H9917772cccYZTJo06aTtV1xxBX/9618ZOnQo27dvP749KiqKZ599lu9+97sMHDiQkJAQbrnlFj++GlZQTVHutQb3jQ1pM2yy5U/Ae3fBj1ZBap/mO69SQUqnKPevVj9FebNN91FT/xmAwHrt6FZKBaegShauie8I3cbAutchiGpqSilVTZOFr2ReDDlb4NDph70ppeonmJrIA0ljX1dNFr6SMR0kVJuilPKBqKgocnNzNWH4mDGG3NxcoqKiGvxcvc7CV2JTofs4e4HexN/YmWmVUo3SuXNnsrKyyM7OdjuUoBMVFUXnzp0b/DxNFr6UORMW3A77v7TLryqlGiU8PJzu3bu7HYbyos1QvtRvCoSEa1OUUiroaLLwpTZtoedEWD8fvC7tV0qplk6Tha9lzoT8vZC1yu1IlFLKZzRZ+FrfCyE0UpuilFJBRZOFr0XFQ+9JTlNUldvRKKWUT2iy8IfMmVB0EHZ/5nYkSinlE0GVLERkqojMzs/PdzeQPhdAeBttilJKBY2gShauTSRYU0SMTRgb3oQq3y0+opRSbgmqZBFQMi+BY7mw8yO3I1FKqSbTZOEvvc6DyHhtilJKBQVNFv4SHmWH0W58CyrL3Y5GKaWaRJOFP2VeAqX5sP1/bkeilFJNosnCn3pMgKhEbYpSSrV4miz8KSwCMqbCpnegosTtaJRSqtE0Wfhb5kwoL9SmKKVUi6bJwt/Sx9qmqI1vuR2JUko1miYLfwsNt6OiNr8DVRVuR6OUUo2iyaI59J9mR0XtXOZ2JEop1SiaLJpDj3MgPEabopRSLZYmi+YQHgV9JsOmt3XacqVUi6TJorlkTIPiw7B3hduRKKVUgwVVsgiYKcpr03uSXUFPm6KUUi1QUCWLgJmivDaRcdBzok0WxrgdjVJKNUhQJYuA138a5O+F/V+6HYlSSjWIJovm1OcCkFBtilJKtTiaLJpTm7bQfSxsXKBNUUqpFkWTRXPLmAa52yB7k9uRKKVUvWmyaG79LgJEm6KUUi2KJovmFtcBupwBGxa4HYlSStWbJgs39J8Gh9bCkR1uR6KUUvWiycIN/abY3xsXuhuHUkrVkyYLNyR1g46D7agopZRqATRZuCVjGmStgoL9bkeilFJ10mThloxp9vemt92NQyml6kGThVtS+0BKX9jwptuRKKVUnTRZuKn/NNj9KRTnuh2JUkqdliYLN2VMBeOx63MrpVQA02Thpg6DILGrjopSSgU8TRZuErEd3TuWQmkALtiklFKOoEoWAb1S3qlkTIOqctj6gduRKKXUKQVVsgjolfJOpfNIiO2go6KUUgEtqJJFixQSAhlTYNtiKD/mdjRKKVUrTRaBIGMqVByD7f9zOxKllKqVJotA0G0MRCfpqCilVMDSZBEIQsOh70Ww+T2oLHc7GqWU+hZNFoEiYyqU5cOuZW5HopRS36LJIlD0mAARcbqCnlIqIGmyCBThUdBnsp2F1lPldjRKKXUSTRaBJGMqHMuBPcvdjkQppU6iySKQ9JoEYVE6KkopFXA0WQSSyFjoeS5sfAuMcTsapZQ6TpNFoMmYCgX7YP8atyNRSqnjNFkEmr4XQEiYjopSSgUUTRaBJjoJuo+z/RbaFKWUChCaLAJRxlQ4sgMOb3Q7EqWUAjRZBKZ+UwDRUVFKqYChySIQxbaDrmfaUVFKKRUANFkEqoypcGgd5G53OxKllNJkEbAyptrfWrtQSgUATRaBKrELdBqqyUIpFRA0WQSyjKmw7wvI3+d2JEqpVk6TRSDLmG5/b1robhxKqVZPk0UgS+kFqRnaFKWUcl29koWI/J+IxIv1tIisEZHJ/g6uoURkqojMzs/PdzsU38mYCrs/heIctyNRSrVi9a1ZXG+MKQAmA6nA94EH/BZVIxlj3jLG3JSQkOB2KL7TfxoYj10USSmlXFLfZCHO7wuBZ40xX3ttU/7UPhOS0rUpSinlqvomi9UisgibLN4XkTjA47+w1HEitilqx1IoyXM7GqVUK1XfZHEDcBcw0hhzDAjHNkWp5pAxHTwVsHWR25EopVqp+iaLM4HNxpg8Ebka+DUQRL3I1idbc9h8sNDtML4tbTjEddSJBZVSrqlvsngCOCYig4E7gd3A836LygXllR7umvcN1zy9gr1HjrkdzslCQuxMtFsXQ3mx29EopVqh+iaLSmOMAaYD/zTG/BOI819YzS8iLISnZ42krNLD1U+v4HBhqdshnaz/NKgsgW1L3I5EKdUK1TdZFIrI3cA1wNsiEorttwgqfTvE8ez3R5JdWMa1T68k/1iF2yGd0PUsiG6ro6KUUq6ob7K4HCjDXm9xEEgD/uq3qFw0rGsSs68ZwY7sYq7/zyqOlVe6HZIVGgb9LoQt70FlmdvRKKVamXolCydBvAQkiMgUoNQYE1R9Ft7O7p3CI1cO4cs9R7nlxTWUVwbIKOGM6VBWADuXuR2JUqqVqe90H5cBK4HvApcBK0TkUn8G5rYLMjvywMxBLNuSzU9e+4oqj3E7JOgxHiLidFSUUqrZhdXzcfdgr7E4DCAiqcBiYK6/AgsEl43sQn5JBfe9s5H4qHD+fHEmIi5euB4WCX3Ot1N/XPR32zSllFLNoL59FiHVicKR24Dntmg3juvBrRN6MmflHh58f7Pb4dhRUcdyYc/nbkeilGpF6vvV9D0ReR+Y49y/HHjHPyEFnl+c35e8kgqeWLqdxOhwbh7f071gep0HYdF2VFT3se7FoZRqVerbwf0LYDYwCBgMzDbG/NKfgQUSEeHe6ZlMGdSR+9/dxGur9roXTEQM9DrXJgtPgHS8K6WCXr2bkowxrxtjfmqM+Ykx5g1/BhWIQkOEhy8bwpk9krl34QYqqlz8oM6YBoX7Yf8a92JQSrUqp00WIlIoIgW1/BSKSEFzBRkoIsJCmHVWNwrLKvlyT557gfQ5H0LCYe1/3YtBKdWqnDZZGGPijDHxtfzEGWPimyvIQHJWrxRCQ4RlW7LdCyI6ETIvgRVPwpqgvdxFKRVAWsWIJl+KjwpnWNdEPnIzWQBM/aft7F7wY/jyRXdjUUoFPU0WjTC+Typr9+WTU+TitBvhUXD5S9BjArx5G3z1snuxKKWCniaLRhjfpx1g179wVXgUXDnHXtk9/1b4+lV341FKBS1NFo0woFM8yTER7jdFAYRHwxVz7DUX82+Bb7TTWynle5osGiEkRBjbO4VlW7LxBMKcURFt4MpXoNsYeOMmWBvUs7AopVygyaKRxvVJJbe4nA0HAmQEcUQMXPUqdD0T5t0E6+a5HZFSKohosmiksb1TAQKjKapaRAxc9Rp0GQWv/wDWz3c7IqVUkNBk0UipcZFkpsUHVrIAiIyF7/0XOo+A12/QlfWUUj6hyaIJxvVOZc3uoxSUBtDyqwCRcfC9udBpGPz3OjuluVJKNYEmiyYY3yeVSo/hs225bofybVHxcPVc6DgEXpsFm991OyKlVAumyaIJhnVLIjYyjGVbA6wpqlpUAlwzDzoMhFevgS3vux2RUqqF0mTRBOGhIZzVM5mPNmdjTAAMoa1NVAJc8wa0HwCvXg1bP3A7IqVUC6TJoonG901lX14JO3KK3Q7l1KIT4dr50C4DXvkebFvsdkRKqRZGk0UTjaseQrs5QJuiqkUnwTXzIbUPzLkKti1xOyKlVAsSVMlCRKaKyOz8/PxmO2eXtm3okRoTeENoa9OmLVy7AFJ6wytXwfYP3Y5IKdVCBFWyMMa8ZYy5KSEhoVnPO653Kit25lJaUdWs522U6oTRtifMuRJ2fOR2REqpFiCokoVbxvdNpbTCw8qdR9wOpX5ikmHWAkhKh5cvh50fux2RUirAabLwgdHdk4kIC2kZTVHVYlJg1luQ1A1evgx2fep2REqppvBUQfkxvx1ek4UPREeEckb3tu4utdoYsak2YSR0hpe+C7s/dzsipVRjrX4OHhsFBfv9cnhNFj4yvk8qWw8XsT+vxO1QGia2nU0Y8R3hpUthz3K3I1JKNVRRNiz5g21ajuvol1NosvCRcX3sENoWV7sAiOsAsxZCbHt48VLYu8rtiJRSDbH4d7YJ6qK/gYhfTqHJwkd6t4ulY0JUy+q38BbfEa5baJumXpwJWV+4HZFSqj52fw5fvQRn3Qapff12Gk0WPiIijO+Tyifbcqis8rgdTuPEd7I1jDbJ8MLFsG+12xEppU6nqgLe/ikkdIFxv/DrqTRZ+NC4PqkUllby1d48t0NpvIQ0W8OIToLnL4Z9a9yOyPJ44MgO2LvS3lZKwYp/weENcMEDdvEzPwrz69FbmTG9UggNET7aks2I9LZuh9N4CZ1twnjuInhhhr2Ir9OQ5jt/RYn9Bzi4Fg6us78PrYPyIru/xzkw43FbE1KqtSrYD0vvh96Tod9Ffj+dJgsfSogOZ0iXRJZtyeZnk/3XdtgsErvaJqnnLoLnp9sRUx0H+f48Rdlw8BsnMThJIWcLGKf2EBEHHTJhyFV2qvWyIvjfvfD4aLjoYRh4qe9jUqoleP8e8FTCdx70W6e2N00WPja+Typ/X7yF3KIykmMj3Q6naZK62RrGsxfB89NswugwsHHH8lRB7nY4tPZEYji4FooOnXhMQhd7/Ixp9neHgZDYDUJqtJb2OR/euNkuG7vpbTsCpE0Lrskp1VDbP4T182DCr6Bt92Y5pQTsOgxNMGLECPPFF+6M5vl6bx7TH/uUf14xhOlD0lyJweeO7IDnptjmoesW2rUxTqesyGlG+sarGWk9VDrXoISEQ2q/EwmhQya0z2zYB35VJXz6d1j6AMSkwvRHodd5jS+jUi1FZRk8cZatff/wcwiP8slhRWS1MWbEqfZrzcLHMtMSSGoTzkdbsoMnWbTtYWsVz02B/0yzCaNdBhgDhQechPDNiWak3O2A8yUkKtEmhBHfP5EcUvpCWETTYgoNs6M/ek2ytYwXL4GRP4BJf/R7R59SrvrsEcjdBle/7rNEUR+aLHwsNEQY2zuVZVty8HgMISH+b0tsFsk9nSapC23S6JBpk8Mxr/XHk9JtMhh0ua0pdBhoO8v92Z7aaQjc9JHtx/j8MVs9nzkbOp/yC5JSLdfRXbDsIeg/vdlr0pos/GBcn1QWfL2fjQcLGNCpeadL96vqhDH3eijJg74XQodBNim0HwBR8e7EFR4F598HfS6A+T+EpyfB2J/B+F9CaLg7MSnlD+/eBRIK59/f7KfWZOEH43qnAPDRluzgShZgF066JUCnNO8+Fn74qf2HWvZX2LoILp4N7fq5HZlSTbfpHdjyLky6114P1cz0ojw/aBcfRf+O8YG/1GowikqAi5+Ay16A/Cz41zj4/HG9kE+1bOXH4N1fQmoGjP6hKyFosvCTcX1SWb37KEVllW6H0jr1n2ZHivQ8B96/G16YDnl73Y5Kqcb5+G+Qv8cOE3epaVWThZ+M75NKpcfw2bYct0NpveLaw5WvwNRH7LQlT5wFX79iR3Ep1VLkbIVP/wmDroD0Ma6FocnCT4Z3SyImIrTlzkIbLERg+Cy45RNo198Os33tWijOrfu5yj371th5j0ry3I7EXcbAOz+H8DYw+V5XQ9Fk4ScRYSGc2TOFj7ZkE4wXPrY4bbvD99+B834Pm9+104Vsed/tqJQ3jwe2LLJDs586B969E54cC3tWuB2Ze9bPgx1L4dzf2IXKXKSjofxofN9UFm88xM6cYnqkxjbpWEeLy9mRU8T27GJ2ZBezM6eI+KhwxvZJ5exeKbSNaeJFbq1BSCic/RM7Pn3ezXbt8eHXweT7ILJp749qgsoyWPtf+Oz/QfYmiE+DyX+yw7Lf+jE8+x07DHrcz+172FqUFsB7v4KOg2HE9W5Ho8nCn8b3PrF6Xn2SRXmlhz1Hio8nhB3ZRezIsb+PHqs4/rjwUKFL2zbkFpXz39VZiMDAtATG9k5hbO9UhnVNIiJMK42n1GEg3PQhfHgffPqI/eZ28b+g62i3I2tdSvJg9bO2uanwgL2Q8+LZkDnzRCfuzR/bZpilf7bv08zZkNjFzaibz9IH7NxpV7wcEElS54bys3MeWkp6chue/f4oAIwxZBeVOcng5ISw92gJVZ4T70dKbCQ9UmPomRpDj5RYeqTG0CM1li5J0YSFhlDlMXyTlcfHW3P4eGs2a/bkUeUxxESEcmbPZMb2TmVcn1TSk9sgzTArZYu0+zPbj5GfBWPugAl3N30qEnV6+Vmw/AlY/R8oL4QeE+CsH0PPiae+2v/rV+Htn9lJJac+AgNmNGfEze/gOjvse9g1MPWfzXLKuuaG0mThZ79fsJ5XVu3hggEd2JljE0Sh13DayLAQuqfE2ETglRB6pMYQH9WwIXIFpRV8vj2Xj7dm8/HWHHbnHgOgc1K0TRy9UzirVwoJ0XpV80nKCuG9u+HLF6D9QPvttX1/t6MKPgfX2qamda/bjtvMS+Cs2+s/9f2RHfD6D+wKjkOvge/8JTjnAfN4bNNb7la47Ytmm1FZk4XLVu06wpWzl5MaF/nthJASQ1pitN/mj9qdW8yyrTl8vCWbz7bnUlRWSYjAkC6JTq0jhcGdEwkL1SYrwF4h+9aPoTQfJv4GzvxRQFT/WzRjYMeHTnPfhxARC8Nm2QvLGtOcVFVhF/z5+GE7/cwlTzfvwlzN4cuX4M1bYdqjtmbRTDRZBIBAmFCwosrDV3vz+HhLNsu25vBNVh4eA3FRYYzpmcLYPimM651Kl7ZtXI3TdcU58Nb/waaF0G0MzHjCruuhGqaqAta/YWdIPbgWYtvDGbfY2Yejk5p+/J3L7CCF4mw7wm30rd9e96QlOnYEHh0Byb3g++81a5k0Waha5R0r59Nttslq2ZZs9ueXAtA9JeZ4R/mZPZOJjWyFYyCMga9ettMrAHznARjyvWZZjazFKyu0fRHLn4CCLDsd/Vm3w6DLIMzHi4EdOwILbreJvee5NrHHtfftOZrbwp/A6ufg5mWNX2iskTRZqDoZY9iRU8yyLbav4/PtuZRUVBEWIgzrmsS4PjZ5ZKYlEBosU67Xx9HdMP9W2P0J9L3IdjTGprodVWAqOAArnoQvnoWyfOh2Noz5sV1vxJ/fjo2xI6re+5Ud/jzjCeg9yX/n86d9q+Gpc20T3QXNP6usJgvVYGWVVazZnceyrdl8vDWbdfsKAEhsE86YXimM753Kef3bt45rOzweWP44LPkjRMbBtEeg30VuRxU4Dm+Ezx6Fb14FU2WXxB3zY0gb3sxxbLLL7B5aB2f80DZNNePCQE3mqYKnJkLhQbhtlSvT/WuyUE2WW1TGJ9tyWLbFDtE9XFhGeKgwsV87Lh3ehQl9UwkP9k7yQxvgjZts+/vQq+16Am6t3+E2Y2DXJ7Y/YusiCIu2HbGjb2229aBrVVEKi39nazjtB8KlT0NqX/fiaYiVT9nrSS55GgZe6koImiyUTxlj2HCggDfW7GP+V/vIKSonOSaCGUPTuHR4ZzI6BvEHaGU5fPQAfPJ3uwLgjCddndit2VVVwsYFNkns/xLapMAZN9vlbJtpeGe9bHnfNh+WF9vmnOHXBXZ/U9Fh+H8joNNguHaBa7FqslB+U1Hl4aPN2cxdncWSTYeoqDIM6BTPJcM6M31IJ5JjfdyhGSj2rLAX8h3dBWfdZofZ+rrzNpCUF9vhnJ8/Cnm7oW1PW+7BV0J4tNvR1a7wILxxix2umzHVXsgXSAnN2xu3wNq58MPPILWPa2FoslDN4mhxOQu+3s/c1Vms3ZdPWEh1M1VnzunXLviaqcqKYNGvbedqu/72Qr5mHr3iFx6PnWLi6C6bGA6tgy9fhJKj0OUMe6V13wtbxjBVjweWPwaL/wAxqfY96j7W7ahOtutTeO5COPuncN7vXA1Fk4VqdpsPFvL6mizmrdlHTlEZyTERTBvSiUuHdw6+ZWa3LIIFt9lhnOf8Csb8X+BfyFdWZBPB0V01fnbb7ZWlJx4rITY5nPVj6HqGO/E21f4vYe4N9grwsT+DCXcFxtrsVRV2Vt3yYvjRCohw9xonTRbKNZVVHpZttc1UizccprzKQ0bHeC4dbpupUoKlmao4F97+CWx4E7qMtsu6tu3hXjyeKijY9+1EUH37WI0FuSLj7YWHSemQ6PxO6u7c7xIcTWxlRfDeL20tqfNImPmUu53xYKc+WfRrO1FgAIyw02ShAkLesXLecpqpvs6yzVQT+tpmqon92rX8WXKNgW9eg3d+AZ5KuODPdloLf3VWlhw9dTLI32tjqCahkNgFk5ROVXxXKuO7URrXhdKYLhTFdKYkJJ7SKg9lFR5KK6ooqzzxu9Lj4cweyfRuH+efcjS3dfPgrTvAeGDKw/ZiQTfk74NHR0L62XDVqwHRAa/JQgWcLYcKeX11FvO+3Ed2YRlJbcKZPiTNaaaKb9kz5OZnwfwf2uko+lxgO1ZrXFXs8RjKKj2UVVZRWlH777LSUkIL9xKWv4fIwj1EFe8lpngfsSVZJJTuI6qq6KRjFoYkcCi0AwdDOrA/pB17TXv2mFR2VrVjb2UixZVCeaWn0cUamZ7ElaO6cuHAjkSFB3gzW13y9sDrN8Le5Xap0gv/2vzDoF+bBVveg1uXu1/DcWiyUAGrssrDx9tymLs6iw/WH6K8ykO/DnFOM1UaqXGB2fxRVllF/rEK8ktO/OR53S84VsbA/a8y5fC/KCWKh6Nu5e3KkfbbeoWH8ioPYEihgK5yiC5ymK5ymC6STdeQw3SRw3TkCCFy4n+zzISz16Syx7Rjv7TjQEgHDoV2ICe8I7nhHTERsUSGhRIVHnLS78iwEKLC7e/I8JPv1/U7MtxOg//2Nwd4ZdVeduYUEx8VxsxhnblyVFf6dmjBtY2qSvj4IfjoL5DYFS55Bjo304WE25bAizPhnHtg/J3Nc8560GShWoS8Y+W89c0B20y1N4/QEOGcvqlcMqwzEzPaERnm22+zFVWekz7sT/vhX1JBXkn58fulFaf/hh4XFUZCdDiZEQe589jD9KjYytcJEymJTCG5/ABJ5ftIKN1PuKf0pOeVRrejLLYrFfFdqIrvhklKR5LSCWmbTkRiJyIjwogIDXFlUkpjDMt3HGHOyj28t+4g5VUehneztY2LBnYkOqKF1jb2LLfTnhcecAYo3OHfAQoVpfDEmYDYobIBdJW5JgvV4mw7XMjc1fuYtyaLw4VlJLYJZ/rgTlw6vAuZaSeaqao8hoLqD/eSihof/uW1fvhXP764vOq0McREhJLYJoL46HASou2Hf2J0BAltwkmIDic+OpzEaHu7+iexTThxUeEnz59VVQHL/gof/w3CopzO43SvjuT0Ex3JgXrNQg1HisuZtyaLl1fuYUd2MXFRYcwcmsYVo7q2zIsyS/Jg4R12ltz0sXaIbXwn/5zrowftCo1Xz4Ne5/rnHI2kyUK1WJVVHj7ZlsPra/bx/vqDlFd66JxkP1Dzj1WctIhUbaLCQ+wHfLTXB3ybb3/Ax3vfdx7n8+tCKsvtcM2W3B9TgzGGlTttbeOddfb9GdIlkatGdWXK4I60iWhBMxYbA1+9BO/caVdKnPYoZEzx7TmO7ITHR9u+rMv+49tj+4AmCxUU8ksqWPjNfj7ekkObiNCTP+BrJIDqb/++brpSp3a0uJx5X+5jzso9bDtcRFxkGNOHduLKUV1b1rU1Odvg9evhwNcw4nqYfJ9vrn8wBl6+3M6pddsqSEhr+jF9TJOFUqrZGGP4YvdR5qzYw8K1Byiv9DC4cwJXjurK1MGdiGkJ66NUlsP//mivg0jtZyf365DZtGNuehteuQom/8mu7xGANFkopVyRd6ycN5zaxpZDRcREhDJ9aBpXjepKZloLqG1s/5+dt6kkDybfC6NualwzYnkxPHaGXVL2lo8D4+rxWmiyUEq5yhjDmj1HeXnFXhZ+s5+ySg8D02xtY9qQToG9GmNxjp3Bduv70Pt8mPE4xKQ07BiL/wCfPAzffxe6neWfOH1Ak4VSKmDkl1Tw5lf7eHnFHjYdLKRNRCjTh9i+jYFpCYF5QaYxsHI2LPoNRCfCxU9Cz4n1e272FnjiLLtGxcVP+jXMptJkoZQKOMYYvtqbx5yVe3jr6wOUVFQxoFM8V47qyvQhnYiLCsCmmoPr7Gp82Ztsv8PE39qRU6diDDw/zXaW3/YFxLZrvlgbQZOFUiqgFZRW8OZX+3l5xR42HiggOjyUaYM7cfmoLgztkhhYtY3yY7DoHvjiGeg4xHZ+p/Sq/bFr59rkcuFDMOrGZg2zMTRZKKVaBGMM32TlM2flHhZ8vZ9j5VV0SojivP7tmdy/A2f0aBs466JsXGinpq8shwsfhCHfO7nzu7TAThQY1wFu/F/gT1uPJgulVAtUWFrBe+sO8sGGQyzbmk1phYf4qDDO6deOyf07ML5vqvsd4/n77IqJuz6GARfDlH/YPg2A9+6G5U/AjUsgrZnmnGoiTRZKqRatpLyKT7blsGj9QZZsOsyR4nIiQkM4q1cyk/t34LyMdrSLd2mOJU8VfPoP+N99doqQS/4NETHwr3F2ivqp/3AnrkZo8clCRGYAFwHtgMeMMYvqeo4mC6WCU5XHsHr3URatP8iiDYfYc+QYAEO7JjLJaa7q1S62+QPL+sL2T+TtgbhOUFliO7UDdd3vWriaLETkGWAKcNgYk+m1/QLgn0Ao8G9jzAP1OFYS8JAx5oa6HqvJQqngZ4xhy6EiPthgE8c3WfkA9EiNOZ44hnZJbL5ZeksL7OJX37wCM56AIVc1z3l9xO1kMQ4oAp6vThYiEgpsASYBWcAq4Eps4ri/xiGuN8Ycdp73N+AlY8yaus6ryUKp1udAfgmLNxxi0YZDfL49l0qPISU2kkn9bT/HmT2Tm2fhpoIDEN/R/+fxMdeboUQkHVjolSzOBH5vjDnfuX83gDGmZqKofr4ADwAfGGMWn+Y8NwE3AXTt2nX47t27fVkMpVQLkl9SwdLNh1m04RAfbc6mqKySNhGhTOibyqT+7ZnYtz0JbQLwWg4X1ZUs3BhOkAbs9bqfBZxxmsffDpwHJIhIL2NMrZdBGmNmA7PB1ix8FKtSqgVKiLZL9U4fkkZZZRWfb8/lgw2H+GDDId5Ze5DQEOGM7m2Z3L89kwZ0IC2xZawl4iY3ahbfBc43xvzAuX8NMMoY47OpGLUZSilVG4/H8HVWHh84zVXbDtu1zAd0imdy/w5M6t+ejI5xgXUhYDMJxJpFFtDF635nYL8LcSilWpmQEGFo1ySGdk3izgv6sSO76Hji+MeSLfx98RY6J0UfTxwj05MIC5QLAV3mRs0iDNvBfS6wD9vBfZUxZr2vzqk1C6VUQ2UXlrFko22q+nhbDuWVHhLbhDPRuRBwXJ+UlrX6XwO5PRpqDjABSAEOAb8zxjwtIhcC/8COgHrGGHOfL8+ryUIp1RTFZZUs25LNBxsOsWTTYfJLKogMC2Fs7xQm9+/AxIx2pMRGuh2mT7k+GsoNmiyUUr5SUeVh1c4jLHI6yPfllSACI7olHb+eIz0lxu0wm0yThVJK+Ygxhg0HCli03iaODQcKAOjdLpbJA2ziGJiW0HwXAvqQJgullPKTvUeOHR+Su3LXEao8hnZxkQzrmkRmWjwD0hLI7JRAalzgN1m1qmQhIlOBqb169bpx69atboejlGpFjhaX879Nh1m6JZt1+/LZmVN8fF/7+EgyOyUwIC2BgWkJZKbF0yE+KqCG6LaqZFFNaxZKKbcVlFawYX8B6/bls975vT27CI/zkZscE+HUPOLJdGogXdpGu5ZAAvE6C6WUCnrxUeGM7pHM6B7Jx7cdK69k44FC1u/PZ92+fNbtK2D2sh1UOhkkPiqMAZ1szSMzLYHMtAS6J8cERB+IJgullGombSLCGN4tieHdko5vK62oYsuhQtbtK2Dd/nzW78vnP5/vprzSA0BMRCj9O8U7ScQmkl6psc1+saA2QymlVICpqPKw7XDRSU1Y6/cXUFJRBUBkWAj9OsYzMC2eTCeJ9G4fS2RY42fV1T4LpZQKAlUew86cYqf5Kt+phRRQWFYJQHio0Kd9HC/fOJqE6IbPqKt9FkopFQRCQ4Re7WLp1S6WGUPTADsx4t6jx443Ye3ILiI+yj8f65oslFKqhQoJEbolx9AtOYaLBvl3wSWdTlEppVSdNFkopZSqU1AlCxGZKiKz8/Pz3Q5FKaWCSlAlC2PMW8aYmxISEtwORSmlgkpQJQullFL+oclCKaVUnTRZKKWUqpMmC6WUUnUKyuk+RCQb2N2Ap6QAOX4KJ1C1xjJD6yx3aywztM5yN6XM3YwxqafaGZTJoqFE5IvTzYkSjFpjmaF1lrs1lhlaZ7n9WWZthlJKKVUnTRZKKaXqpMnCmu12AC5ojWWG1lnu1lhmaJ3l9luZtc9CKaVUnbRmoZRSqk6aLJRSStWpVScLEblARDaLyDYRucvteJpCRLqIyIcislFE1ovI/znb24rIByKy1fmd5PWcu52ybxaR8722DxeRtc6+R0RE3ChTfYlIqIh8KSILnfutocyJIjJXRDY57/mZraTcP3H+vteJyBwRiQq2covIMyJyWETWeW3zWRlFJFJEXnW2rxCR9HoFZoxplT9AKLAd6AFEAF8D/d2Oqwnl6QgMc27HAVuA/sCDwF3O9ruAvzi3+ztljgS6O69FqLNvJXAmIMC7wHfcLl8dZf8p8DKw0LnfGsr8H+AHzu0IIDHYyw2kATuBaOf+a8B1wVZuYBwwDFjntc1nZQRuBZ50bl8BvFqvuNx+YVx8Q84E3ve6fzdwt9tx+bB8bwKTgM1AR2dbR2BzbeUF3ndek47AJq/tVwL/crs8pylnZ2AJMJETySLYyxzvfGhKje3BXu40YC/QFrsk9EJgcjCWG0ivkSx8Vsbqxzi3w7BXfEtdMbXmZqjqP7xqWc62Fs+pVg4FVgDtjTEHAJzf7ZyHnar8ac7tmtsD1T+AOwGP17ZgL3MPIBt41ml++7eIxBDk5TbG7AMeAvYAB4B8Y8wigrzcDl+W8fhzjDGVQD6QXFcArTlZ1NZG2eLHEYtILPA6cIcxpuB0D61lmznN9oAjIlOAw8aY1fV9Si3bWlSZHWHYZoonjDFDgWJs08SpBEW5nXb66djmlk5AjIhcfbqn1LKtxZW7Do0pY6PK35qTRRbQxet+Z2C/S7H4hIiEYxPFS8aYec7mQyLS0dnfETjsbD9V+bOc2zW3B6IxwDQR2QW8AkwUkRcJ7jKDjTfLGLPCuT8XmzyCvdznATuNMdnGmApgHnAWwV9u8G0Zjz9HRMKABOBIXQG05mSxCugtIt1FJALb0bPA5ZgazRnp8DSw0RjzsNeuBcAs5/YsbF9G9fYrnJER3YHewEqnilsoIqOdY17r9ZyAYoy52xjT2RiTjn3//meMuZogLjOAMeYgsFdE+jqbzgU2EOTlxjY/jRaRNk685wIbCf5yg2/L6H2sS7H/N3XXrNzuyHG5E+lC7Kih7cA9bsfTxLKcja1KfgN85fxciG2LXAJsdX639XrOPU7ZN+M1GgQYAaxz9j1KPTq/3P4BJnCigzvoywwMAb5w3u/5QFIrKfcfgE1OzC9gRwEFVbmBOdg+mQpsLeAGX5YRiAL+C2zDjpjqUZ+4dLoPpZRSdWrNzVBKKaXqSZOFUkqpOmmyUEopVSdNFkoppeqkyUIppVSdNFmoFkVEjIj8zev+z0Xk9z469nMicqkvjlXHeb7rzBT7YY3tIc7soOuc2UJXOWPn/RnLLhFJ8ec5VHDQZKFamjJgZqB9wIlIaAMefgNwqzHmnBrbL8dOYzHIGDMQuBjI802ESjWNJgvV0lRi1xn+Sc0dNWsGIlLk/J4gIh+JyGsiskVEHhCR74nISucbfE+vw5wnIh87j5viPD9URP7qfNP/RkRu9jruhyLyMrC2lniudI6/TkT+4mz7LfYCyidF5K81ntIROGCM8QAYY7KMMUed5z0hIl+IXcvhD17n2CUifxaRz539w0TkfRHZLiK3eMW5TETeEJENIvKkiHzrf19ErnZek69E5F9OuUOd17W6tvOt1121DmFuB6BUIzwGfCMiDzbgOYOBDOwcODuAfxtjRoldJOp24A7ncenAeKAn8KGI9MJOlZBvjBkpIpHApyKyyHn8KCDTGLPT+2Qi0gn4CzAcOAosEpEZxpg/ishE4OfGmC9qxPga8ImIjMVepfuiMeZLZ989xpgjTg1miYgMMsZ84+zba4w5U0T+DjyHnTMrClgPPOkVZ39gN/AeMBM7p1R1vBnYms0YY0yFiDwOfM85RpoxJtN5XGKdr7QKSlqzUC2OsbPpPg/8uAFPW2WMOWCMKcNOf1D9Yb8WmyCqvWaM8RhjtmKTSj/smgnXishX2Gnfk7Fz8ICdh+ekROEYCSw1dtK7SuAl7KI2pytXFtAXu0aBB5sUznV2XyYia4AvgQHYD/5q1XOarQVWGGMKjTHZQKnXh/tKY8wOY0wVdjqJs2uc/lxsYlvllPNc7FToO4AeIvL/ROQC4HQzGasgpjUL1VL9A1gDPOu1rRLnC5AzeVqE174yr9ser/seTv4/qDn/TfV0z7cbY9733iEiE7DTg9emUct0OsnsXeBdETkEzBCRHcDPgZHGmKMi8hy25lDNuyw1y1ldttrKVTPe/xhj7v5WQUQGA+cDPwIuA65vaLlUy6c1C9UiGWOOYJttbvDavAv77RjsugfhjTj0d51RST2x36w3Y1cW+6HYKeARkT5iFxs6nRXAeBFJcZqOrgQ+Ot0TnP6GTs7tEGAQttkoHpuU8kWkPfCdRpRrlNgZlkOwzU2f1Ni/BLhURNo5528rIt2cgQQhxpjXgd9gp0JXrZDWLFRL9jfgNq/7TwFvishK7Iffqb71n85m7Id6e+AWY0ypiPwb21S1xqmxZAMzTncQY8wBEbkb+BD7rf0dY0xd02C3A55y+kXAzgj6qBPDl9j+gx3Ap40o1+fAA8BAYBnwRo14N4jIr7F9KyHYGU9/BJRgV+Sr/mL5rZqHah101lmlgpzTXPZzY8wUl0NRLZg2QymllKqT1iyUUkrVSWsWSiml6qTJQimlVJ00WSillKqTJgullFJ10mShlFKqTv8f6lg/Lmtpvo0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "samples_num = [100, 400, 800 ,1500, 2500, 3500, 5000, 6500, 7500, 9000, 10000]\n",
    "loss_per_samples_num = []\n",
    "val_loss_per_samples_num = []\n",
    "\n",
    "# Looping over varies samples numbers, and applying learning algorithm for each.\n",
    "for M in samples_num:\n",
    "    # Creating the model's architecture\n",
    "    model = create_FNN_model()\n",
    "\n",
    "    # Setting the optimization parameters -MAE loss function and SGD optimzier.\n",
    "    model.compile(loss='mean_absolute_error', optimizer='Adam')\n",
    "\n",
    "    # Running learning process\n",
    "    history = model.fit(x_train[:M], y_train[:M], validation_split = 0.3, epochs=100, batch_size=64, verbose=0)\n",
    "\n",
    "    # Saving losses\n",
    "    loss_per_samples_num = loss_per_samples_num + [history.history['loss'][-1]]\n",
    "    val_loss_per_samples_num = val_loss_per_samples_num + [history.history['val_loss'][-1]]\n",
    "\n",
    "print(\"Train Loss is: \" + \"{:.2e}\".format(history.history['loss'][-1]))\n",
    "print(\"Evaluation Loss is: \" + \"{:.2e}\".format(history.history['val_loss'][-1]))\n",
    "\n",
    "# Plotting results\n",
    "plt.semilogy(samples_num, loss_per_samples_num)\n",
    "plt.semilogy(samples_num, val_loss_per_samples_num)\n",
    "plt.title(\"Model Loss vs Number of Samples\")\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('Number of Samples')\n",
    "plt.legend(['train', \"evaluation\"], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd9910b",
   "metadata": {},
   "source": [
    "The results provide us 2 observations:\n",
    "1. **Worse training error** - the error decreases as the number of samples increases as expected, but the **results are worse** then those achieved for stationary fault signal (`train loss` is 1.65e-2 compares to 4.9e-3)\n",
    "2. **Worse generalization** - the `evaluation loss` is high compared to the `train loss`, even if we increase the number of samples.\n",
    "\n",
    "Those results presents a weakness of the regular FNN - it's hard to train an FNN to learn a feature that can be present in different variations over different signals. A regular FNN learns not only the *size* of the fault, but also its *location*.\n",
    "That means that it will classify differently faults that appear in different locations of the signal (see **Figure x**). \n",
    "\n",
    "In this case, if we want the algorithm to learn well how to estimate the fault size, it has to learn all the fault sizes for **every possible location**. It requires **a lot of samples** to learn well and generalize well at those conditions, which will be very **inefficient** to calculate.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"img/fault_location.png\" alt=\"bla\" style=\"width: 80%\">\n",
    "  <figcaption><center><b>Figure x</b> - Although both faults are at the same size, a regular FNN will classify them differently because it takes into account the fault's location.</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "This type of problem called `invariance`, meaning that you can recognize an object as an object, even when its appearance varies in some way (in this case - varies in location).\n",
    "\n",
    "### Types of Invariance\n",
    "An object's appearance can vary in many ways, and we need an algorithm that's invariant to those variations. The common invariants are `translation invariance` (varying location), `scaling invariance` (varying scale), `rotation invariance` (varying rotation), and `illumination invariance` (varying illumination), as ilustrated n **Figure x**.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"img/invariance.png\" alt=\"bla\" style=\"width: 60%\">\n",
    "  <figcaption><center><b>Figure x</b> - Common types of invariance problems.</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "In order to avoid this weakness, we'll learn how to solve this problem using convolution network (CNN). Such a network can deal with variances in an efficient way. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee256244",
   "metadata": {},
   "source": [
    "\n",
    "## Part 3 - Convolutional Neural Networks\n",
    "This part will present the basics of CNN. We'll see that this kind of network is more suitable for our problem.\n",
    "\n",
    "For a visual illustration of CNN's principles, I recommend watching following video created by `deeplizard` channel:\n",
    "<br>[Convolutional Neural Networks (CNNs) explained](https://www.youtube.com/watch?v=YRhxdVk_sIs)\n",
    "\n",
    "As usual, we'll use the `keras` library for implementing the model and running the fitting process. For further information about this library, please refer to its documentation site.\n",
    "\n",
    "A CNN network's main goal is **learning the features** more efficiently. For some AI tasks, the features are patterns in the data (such as edges in a picture). Under such assumptions, it's hard for a regular FNN to extract features efficiently\n",
    "\n",
    "It should be emphasized that one should use a CNN only for tasks that can **take advantage of its benefits**. CNN is widely used for vision applications, such as image and video recognition, image classification, image segmentation, medical image analysis, natural language processing.\n",
    "\n",
    "Convolution layer consist of several unit, as shown in **Figure x**:\n",
    "1. Convolution Layer\n",
    "2. Pooling Layer\n",
    "3. Flattening Layer\n",
    "\n",
    "<figure>\n",
    "  <img src=\"img/conv.png\" alt=\"bla\" style=\"width: 40%\">\n",
    "  <figcaption><center><b>Figure x</b> - The layers usually takes part in CNN - convolution, activation and pooling.</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "In the following parts we will discuss each layer.\n",
    "\n",
    "### Convolution Layer\n",
    "\n",
    "Convolution layer performs a `convolution` operation between the data and a convolution `kernel` (or a `filter`). The convolution's goal is to extract features that presents a pattern in the data. The `kernel` represents a weight vector / matrix that corresponds to the type of feature we want to extract. The `kernel` will be much smaller then the data. The output of the convolution layer is called `feature map`, as it contains the extracted features.\n",
    "\n",
    "An example for applying a kernel to extract edges in a picture is shown in **Figure x**. This kernel subtracts each pixel from the pixel that's on his right, so it emphasizes `edges` as can be seen in the `feature map`, the output.\n",
    "\n",
    "\n",
    "<figure>\n",
    "  <img src=\"img/kernel.png\" alt=\"bla\" style=\"width: 60%\">\n",
    "  <figcaption><center><b>Figure x</b> - Applying a kernel in order to extract edges in a picture. Darker color represent lower value while brighter color represents higher value.<br><i>Image taken by Paula Goodfellow</i></center></figcaption>\n",
    "</figure>\n",
    "\n",
    "The `kernels` are actually a weight function as we already know from FNN, but with `shared parameters` for the whole layer. This is because the same kernels weights repeat for every node operation. \n",
    "The `kernel` values are also **learned by the algorithm** with the same back-propagation mechanism that sets the weights of regular layers.\n",
    "\n",
    "Many time the `kernel` is referred as `filter` that is being applied on the data. An important character of `convolution` layers is that they usually calculate several `filters` in parallel (see **Figure x**), when each `filter` creates a different `feature map`. This parallel structure is called `channels`, when each `feature map` is a `channel`.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"img/filters.png\" alt=\"bla\" style=\"width: 50%\">\n",
    "  <figcaption><center><b>Figure x</b> - An illustration of a convolution layer with N filters. The output of the convolution layer is an N-channel structure of N feature maps, corresponding to the N filters applied on the data.</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "An interesting property of the convolution layer is that it's not `fully connected`. A node in the convolution layer is only depends on `K` nodes of the previous layer (`K` stands for `kernel size`). This property means that less computations are required in order to calculate the feedforward and backpropagation steps compared to a `fully connected` layer.\n",
    "\n",
    "At the end of a convolution layer we apply an `activation` as in a `fully connected` layer. **Notice** that sometimes the `activation` process is defined as a separate layer from the `convolution` layer. In the `keras` library, we define the `convolution` and the `activation` in the same layer.\n",
    "\n",
    "\n",
    "### Pooling Layer\n",
    "The next stage \n",
    "Pooling helps to make the representation became approximately invariant to small translations of the input. In other words, **local** translations won't damage. The intuition is that we don't care if the \n",
    "\n",
    "<figure>\n",
    "  <img src=\"img/pooling.png\" alt=\"bla\" style=\"width: 30%\">\n",
    "  <figcaption><center><b>Figure x</b> - Two pooling operations on a given data - (above) max pooling and (below) average pooling.<br><i>Image created by Xianwei Jiang, Mingzhou Lu & Shui-Hua Wang</i></center></figcaption>\n",
    "</figure>\n",
    "\n",
    "Pooling is also a form of `subsampling`. This  \n",
    "\n",
    "### Flattening Layer\n",
    "In cases we work with a multi-dimensional data (such as 2D image), we have to flatten our data at the end of the convolution process. Flattening means transforming the high-dimensional data into 1D vector that can be used as an input to a classification FNN layer. \n",
    "<br>In our case the input data is one-dimensional, but the convolution process creates a `multi-dimensional representation` as  the number of `filters` defined for the convolution layer. For this reason, we still have to flatten the data after processing it with convolution layers. \n",
    "\n",
    "\n",
    "### Convolution Network Summery\n",
    "A example of a convolution network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b21ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "110/110 [==============================] - 22s 196ms/step - loss: 0.0886 - val_loss: 0.0833\n",
      "Epoch 2/100\n",
      "110/110 [==============================] - 21s 195ms/step - loss: 0.0835 - val_loss: 0.0828\n",
      "Epoch 3/100\n",
      "110/110 [==============================] - 21s 194ms/step - loss: 0.0832 - val_loss: 0.0827\n",
      "Epoch 4/100\n",
      "110/110 [==============================] - 21s 194ms/step - loss: 0.0828 - val_loss: 0.0822\n",
      "Epoch 5/100\n",
      "110/110 [==============================] - 23s 211ms/step - loss: 0.0825 - val_loss: 0.0821\n",
      "Epoch 6/100\n",
      "110/110 [==============================] - 22s 203ms/step - loss: 0.0821 - val_loss: 0.0818\n",
      "Epoch 7/100\n",
      "110/110 [==============================] - 21s 191ms/step - loss: 0.0818 - val_loss: 0.0815\n",
      "Epoch 8/100\n",
      "110/110 [==============================] - 20s 186ms/step - loss: 0.0814 - val_loss: 0.0818\n",
      "Epoch 9/100\n",
      "110/110 [==============================] - 23s 207ms/step - loss: 0.0811 - val_loss: 0.0810\n",
      "Epoch 10/100\n",
      "110/110 [==============================] - 22s 197ms/step - loss: 0.0807 - val_loss: 0.0803\n",
      "Epoch 11/100\n",
      "110/110 [==============================] - 23s 209ms/step - loss: 0.0800 - val_loss: 0.0798\n",
      "Epoch 12/100\n",
      "110/110 [==============================] - 22s 201ms/step - loss: 0.0794 - val_loss: 0.0793\n",
      "Epoch 13/100\n",
      "110/110 [==============================] - 20s 186ms/step - loss: 0.0787 - val_loss: 0.0787\n",
      "Epoch 14/100\n",
      "110/110 [==============================] - 21s 188ms/step - loss: 0.0780 - val_loss: 0.0779\n",
      "Epoch 15/100\n",
      "110/110 [==============================] - 22s 196ms/step - loss: 0.0771 - val_loss: 0.0770\n",
      "Epoch 16/100\n",
      "110/110 [==============================] - 23s 206ms/step - loss: 0.0762 - val_loss: 0.0761\n",
      "Epoch 17/100\n",
      "110/110 [==============================] - 22s 202ms/step - loss: 0.0750 - val_loss: 0.0749\n",
      "Epoch 18/100\n",
      "110/110 [==============================] - 24s 220ms/step - loss: 0.0737 - val_loss: 0.0740\n",
      "Epoch 19/100\n",
      "110/110 [==============================] - 23s 208ms/step - loss: 0.0722 - val_loss: 0.0743\n",
      "Epoch 20/100\n",
      "110/110 [==============================] - 23s 206ms/step - loss: 0.0696 - val_loss: 0.0700\n",
      "Epoch 21/100\n",
      "110/110 [==============================] - 22s 205ms/step - loss: 0.0676 - val_loss: 0.0692\n",
      "Epoch 22/100\n",
      "110/110 [==============================] - 21s 191ms/step - loss: 0.0650 - val_loss: 0.0675\n",
      "Epoch 23/100\n",
      "110/110 [==============================] - 21s 192ms/step - loss: 0.0609 - val_loss: 0.0599\n",
      "Epoch 24/100\n",
      "110/110 [==============================] - 21s 192ms/step - loss: 0.0581 - val_loss: 0.0589\n",
      "Epoch 25/100\n",
      "110/110 [==============================] - 21s 191ms/step - loss: 0.0543 - val_loss: 0.0563\n",
      "Epoch 26/100\n",
      "110/110 [==============================] - 21s 192ms/step - loss: 0.0512 - val_loss: 0.0786\n",
      "Epoch 27/100\n",
      "110/110 [==============================] - 21s 187ms/step - loss: 0.0507 - val_loss: 0.0555\n",
      "Epoch 28/100\n",
      "110/110 [==============================] - 21s 193ms/step - loss: 0.0481 - val_loss: 0.0424\n",
      "Epoch 29/100\n",
      "110/110 [==============================] - 22s 197ms/step - loss: 0.0473 - val_loss: 0.0624\n",
      "Epoch 30/100\n",
      "110/110 [==============================] - 21s 192ms/step - loss: 0.0433 - val_loss: 0.0384\n",
      "Epoch 31/100\n",
      "110/110 [==============================] - 22s 198ms/step - loss: 0.0421 - val_loss: 0.0312\n",
      "Epoch 32/100\n",
      "110/110 [==============================] - 21s 194ms/step - loss: 0.0413 - val_loss: 0.0329\n",
      "Epoch 33/100\n",
      "110/110 [==============================] - 22s 197ms/step - loss: 0.0395 - val_loss: 0.0576\n",
      "Epoch 34/100\n",
      "110/110 [==============================] - 22s 198ms/step - loss: 0.0374 - val_loss: 0.0303\n",
      "Epoch 35/100\n",
      "110/110 [==============================] - 21s 190ms/step - loss: 0.0358 - val_loss: 0.0583\n",
      "Epoch 36/100\n",
      "110/110 [==============================] - 21s 192ms/step - loss: 0.0351 - val_loss: 0.0484\n",
      "Epoch 37/100\n",
      "110/110 [==============================] - 21s 190ms/step - loss: 0.0339 - val_loss: 0.0347\n",
      "Epoch 38/100\n",
      "110/110 [==============================] - 21s 195ms/step - loss: 0.0339 - val_loss: 0.0431\n",
      "Epoch 39/100\n",
      "110/110 [==============================] - 21s 189ms/step - loss: 0.0301 - val_loss: 0.0412\n",
      "Epoch 40/100\n",
      "110/110 [==============================] - 21s 193ms/step - loss: 0.0317 - val_loss: 0.0269\n",
      "Epoch 41/100\n",
      "110/110 [==============================] - 23s 211ms/step - loss: 0.0299 - val_loss: 0.0229\n",
      "Epoch 42/100\n",
      "110/110 [==============================] - 21s 191ms/step - loss: 0.0282 - val_loss: 0.0286\n",
      "Epoch 43/100\n",
      "110/110 [==============================] - 21s 192ms/step - loss: 0.0276 - val_loss: 0.0244\n",
      "Epoch 44/100\n",
      "110/110 [==============================] - 21s 194ms/step - loss: 0.0269 - val_loss: 0.0358\n",
      "Epoch 45/100\n",
      "110/110 [==============================] - 24s 216ms/step - loss: 0.0264 - val_loss: 0.0305\n",
      "Epoch 46/100\n",
      "110/110 [==============================] - 22s 198ms/step - loss: 0.0247 - val_loss: 0.0346\n",
      "Epoch 47/100\n",
      "110/110 [==============================] - 21s 187ms/step - loss: 0.0251 - val_loss: 0.0310\n",
      "Epoch 48/100\n",
      "110/110 [==============================] - 21s 193ms/step - loss: 0.0223 - val_loss: 0.0195\n",
      "Epoch 49/100\n",
      "110/110 [==============================] - 21s 194ms/step - loss: 0.0232 - val_loss: 0.0287\n",
      "Epoch 50/100\n",
      "110/110 [==============================] - 22s 204ms/step - loss: 0.0234 - val_loss: 0.0316\n",
      "Epoch 51/100\n",
      "110/110 [==============================] - 22s 196ms/step - loss: 0.0220 - val_loss: 0.0202\n",
      "Epoch 52/100\n",
      "110/110 [==============================] - 21s 192ms/step - loss: 0.0211 - val_loss: 0.0150\n",
      "Epoch 53/100\n",
      "110/110 [==============================] - 23s 213ms/step - loss: 0.0208 - val_loss: 0.0274\n",
      "Epoch 54/100\n",
      "110/110 [==============================] - 23s 209ms/step - loss: 0.0204 - val_loss: 0.0144\n",
      "Epoch 55/100\n",
      "110/110 [==============================] - 21s 195ms/step - loss: 0.0197 - val_loss: 0.0189\n",
      "Epoch 56/100\n",
      "110/110 [==============================] - 23s 209ms/step - loss: 0.0201 - val_loss: 0.0180\n",
      "Epoch 57/100\n",
      "110/110 [==============================] - 22s 204ms/step - loss: 0.0200 - val_loss: 0.0246\n",
      "Epoch 58/100\n",
      "110/110 [==============================] - 22s 201ms/step - loss: 0.0193 - val_loss: 0.0254\n",
      "Epoch 59/100\n",
      "110/110 [==============================] - 21s 195ms/step - loss: 0.0185 - val_loss: 0.0149\n",
      "Epoch 60/100\n",
      "110/110 [==============================] - 21s 195ms/step - loss: 0.0176 - val_loss: 0.0180\n",
      "Epoch 61/100\n",
      "110/110 [==============================] - 21s 195ms/step - loss: 0.0180 - val_loss: 0.0193\n",
      "Epoch 62/100\n",
      "110/110 [==============================] - 22s 203ms/step - loss: 0.0184 - val_loss: 0.0147\n",
      "Epoch 63/100\n",
      "110/110 [==============================] - 25s 224ms/step - loss: 0.0182 - val_loss: 0.0175\n",
      "Epoch 64/100\n",
      "110/110 [==============================] - 23s 204ms/step - loss: 0.0166 - val_loss: 0.0223\n",
      "Epoch 65/100\n",
      "110/110 [==============================] - 22s 198ms/step - loss: 0.0166 - val_loss: 0.0164\n",
      "Epoch 66/100\n",
      "110/110 [==============================] - 22s 198ms/step - loss: 0.0172 - val_loss: 0.0195\n",
      "Epoch 67/100\n",
      "110/110 [==============================] - 21s 190ms/step - loss: 0.0166 - val_loss: 0.0165\n",
      "Epoch 68/100\n",
      "110/110 [==============================] - 21s 191ms/step - loss: 0.0163 - val_loss: 0.0186\n",
      "Epoch 69/100\n",
      "110/110 [==============================] - 21s 189ms/step - loss: 0.0160 - val_loss: 0.0199\n",
      "Epoch 70/100\n",
      "110/110 [==============================] - 21s 192ms/step - loss: 0.0159 - val_loss: 0.0121\n",
      "Epoch 71/100\n",
      "110/110 [==============================] - 21s 190ms/step - loss: 0.0157 - val_loss: 0.0170\n",
      "Epoch 72/100\n",
      "110/110 [==============================] - 22s 201ms/step - loss: 0.0157 - val_loss: 0.0130\n",
      "Epoch 73/100\n",
      "110/110 [==============================] - 27s 242ms/step - loss: 0.0147 - val_loss: 0.0131\n",
      "Epoch 74/100\n",
      "110/110 [==============================] - 24s 218ms/step - loss: 0.0145 - val_loss: 0.0114\n",
      "Epoch 75/100\n",
      "110/110 [==============================] - 21s 189ms/step - loss: 0.0150 - val_loss: 0.0170\n",
      "Epoch 76/100\n",
      "110/110 [==============================] - 20s 186ms/step - loss: 0.0146 - val_loss: 0.0153\n",
      "Epoch 77/100\n",
      "110/110 [==============================] - 21s 189ms/step - loss: 0.0149 - val_loss: 0.0209\n",
      "Epoch 78/100\n",
      "110/110 [==============================] - 25s 225ms/step - loss: 0.0145 - val_loss: 0.0182\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - 22s 200ms/step - loss: 0.0139 - val_loss: 0.0158\n",
      "Epoch 80/100\n",
      "110/110 [==============================] - 21s 195ms/step - loss: 0.0137 - val_loss: 0.0147\n",
      "Epoch 81/100\n",
      "110/110 [==============================] - 20s 185ms/step - loss: 0.0133 - val_loss: 0.0140\n",
      "Epoch 82/100\n",
      "110/110 [==============================] - 20s 184ms/step - loss: 0.0137 - val_loss: 0.0109\n",
      "Epoch 83/100\n",
      "110/110 [==============================] - 20s 184ms/step - loss: 0.0137 - val_loss: 0.0144\n",
      "Epoch 84/100\n",
      "110/110 [==============================] - 20s 186ms/step - loss: 0.0139 - val_loss: 0.0150\n",
      "Epoch 85/100\n",
      "110/110 [==============================] - 22s 198ms/step - loss: 0.0130 - val_loss: 0.0137\n",
      "Epoch 86/100\n",
      " 94/110 [========================>.....] - ETA: 3s - loss: 0.0134"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def create_CNN_model():\n",
    "    # Creating a sequential model - a simple way to construct a model. For constructing more complex models \n",
    "    # with additional flexibility in design, one can use a Functional model.\n",
    "    model = Sequential()\n",
    "\n",
    "    # Adding the input layer with a dimension of the sample_size (1024)\n",
    "    # The input layer doesnt have any more properties.\n",
    "    model.add(keras.layers.InputLayer(input_shape=(sample_size,1)))\n",
    "    \n",
    "    # Adding a convolution layer with kernel\n",
    "    model.add(keras.layers.Conv1D(32, 16, activation='relu'))\n",
    "    model.add(keras.layers.MaxPooling1D(2))\n",
    "    model.add(keras.layers.Conv1D(64, 16, activation='relu'))\n",
    "    model.add(keras.layers.MaxPooling1D(2))\n",
    "    #model.add(keras.layers.Conv1D(128, 10, activation='relu'))\n",
    "    #model.add(keras.layers.MaxPooling1D(4))\n",
    "    #model.add(keras.layers.Conv1D(128, 10, activation='relu'))\n",
    "    #model.add(keras.layers.MaxPooling1D(4))\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Adding a hidden layer with a dimension of the 64 (an arbitrary value)\n",
    "    # The type of layer is Dense, its activation is ReLu and it's weights initialized with normal distribution.\n",
    "    model.add(Dense(16, kernel_initializer='normal', activation='relu', name='hidden_layer'))\n",
    "    \n",
    "    # Adding the output later with the dimension of the output (a scalar)\n",
    "    # The type of layer is Dense, its activation is sigmoid and it's weights initialized with normal distribution.\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid', name='output_layer'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Creating the model's architecture and setting the optimization parameters\n",
    "model = create_CNN_model()\n",
    "model.compile(loss='mean_absolute_error', optimizer='SGD')\n",
    "\n",
    "# Running learning process\n",
    "history = model.fit(x_train[:M], y_train[:M], validation_split = 0.3, epochs=100, batch_size=64, verbose=1)\n",
    "\n",
    "# Printing Results\n",
    "print(\"Train Loss is: \" + \"{:.2e}\".format(history.history['loss'][-1]))\n",
    "print(\"Number of epochs until stopping: \" + str(np.size(history.history['loss'])))\n",
    "\n",
    "# Plotting loss vs epochs\n",
    "plt.semilogy(history.history['loss'], 'r')\n",
    "plt.title(\"Model Loss vs Epochs\")\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7090067b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "samples_num = [100, 400, 800 ,1500, 2500, 3500, 5000, 6500, 7500, 9000, 10000]\n",
    "loss_per_samples_num = []\n",
    "val_loss_per_samples_num = []\n",
    "\n",
    "for M in samples_num:\n",
    "    # Creating the model's architecture\n",
    "    model = create_CNN_model()\n",
    "\n",
    "    # Setting the optimization parameters -MAE loss function and SGD optimzier.\n",
    "    model.compile(loss='mean_absolute_error', optimizer='SGD')\n",
    "\n",
    "    # Running learning process\n",
    "    history = model.fit(x_train[:M], y_train[:M], validation_split = 0.3, epochs=100, batch_size=64, verbose=0)\n",
    "\n",
    "    # Saving losses\n",
    "    loss_per_samples_num = loss_per_samples_num + [history.history['loss'][-1]]\n",
    "    val_loss_per_samples_num = val_loss_per_samples_num + [history.history['val_loss'][-1]]\n",
    "\n",
    "print(\"Train Loss is: \" + \"{:.2e}\".format(history.history['loss'][-1]))\n",
    "print(\"Evaluation Loss is: \" + \"{:.2e}\".format(history.history['val_loss'][-1]))\n",
    "# Plottinh results\n",
    "plt.semilogy(samples_num, loss_per_samples_num)\n",
    "plt.semilogy(samples_num, val_loss_per_samples_num)\n",
    "plt.title(\"Model Loss vs Number of Samples\")\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('Number of Samples')\n",
    "plt.legend(['train', \"evaluation\"], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4523e14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    # Creating a sequential model - a simple way to construct a model. For constructing more complex models \n",
    "    # with additional flexibility in design, one can use a Functional model.\n",
    "    model = Sequential()\n",
    "\n",
    "    # Adding the input layer with a dimension of the sample_size (1024)\n",
    "    # The input layer doesnt have any more properties.\n",
    "    model.add(keras.layers.InputLayer(input_shape=(sample_size,)))\n",
    "\n",
    "    # Adding a hidden layer with a dimension of the 64 (an arbitrary value)\n",
    "    # The type of layer is Dense, its activation is ReLu and it's weights initialized with normal distribution.\n",
    "    model.add(Dense(64, kernel_initializer='normal', activation='relu', name='hidden_layer'))\n",
    "    \n",
    "    # Adding the output later with the dimension of the output (a scalar)\n",
    "    # The type of layer is Dense, its activation is sigmoid and it's weights initialized with normal distribution.\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid', name='output_layer'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6adc3cc",
   "metadata": {},
   "source": [
    "Finally, lets construct the model and print its architecture using `model.summary()`.  \n",
    "<br>*Note that the input layer doesnt appear in the model summery, it starts only from the second layer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d20284",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42026d8",
   "metadata": {},
   "source": [
    "### Part 3.3 - Optimization\n",
    "The optimization defines the algorithm which solves the learning problem. It includes the `loss function` and the `optimizer`. \n",
    "<br>The `loss function` is the error which the algorithm aims to reduce to minimum. We'll set this parameter to `Mean Absolute Error`, or `MAE` in short. <br>The `optimizer` is the algorithm we use in order to minimize the `loss function`. For this type of problem, we'll use the numerical optimizer called `Stochastic Gradient Descent`, or `SGD` in short.\n",
    "\n",
    "#### Mean Absolute Error\n",
    "The `mean absolute error` is defined by following **Equation 2**:\n",
    "<font size=\"3\">$$MAE=\\frac{\\sum_{i=0}^{N-1}{|predicted_i-true_i|}}{N} $$</font>\n",
    "<br><center><b>Equation 2</b> -  Mean absolute error.</center>\n",
    "\n",
    "#### Stochastic Gradient Descent\n",
    "`Gradient descent` is an iterative optimization algorithm for finding a local minimum of a differential function. In our case, we try to minimize the loss function (`MAE`), and the controlled variables are the weights that transforms the input into the output through the networks layers. The algorithm calculates the `gradient` of the function - a vector of partial derivatives for each variable. Then the algorithm calculated the new value of the function by subtracting the gradient from the previous value.\n",
    "The descent rate is controlled by a design parameter $\\alpha$ as can be seen in **Equation 3**. In `kears` library this design parameter is configured by the `learning_rate` parameter, as you'll experience later.\n",
    "\n",
    "\n",
    "<font size=\"3\">$$Loss_m = loss_{m-1}-\\alpha F(Loss_{m-1}) $$</font>\n",
    "<br><center><b>Equation 3</b> -  Calculation of the $m$-step of the gradient descent algorithm for the loss output by subtracting the gradient of the previous output $loss_{m-1}$ from its value. The gradient is multiplied by a step_size parameter $\\alpha$ before subtraction to control the rate and the accuracy of the descent.</center>\n",
    "\n",
    "<br><br>\n",
    "`Stochastic gradient descent` is a version of `gradient descent` that uses only a subset of the samples (called `mini-batch`) in order to calculate the gradient. The advantage is obvious - calculation of less samples *reduces runtime*. The tradeoff is that the descent is less accurate for each step, and therefore can yield worse results. The difference in the descent movement is illustrated in **Figure 6**. \n",
    "\n",
    "<figure>\n",
    "  <img src=\"img/sgd.png\" alt=\"bla\" style=\"width: 80%\">\n",
    "  <figcaption><b>Figure 6</b> - Gradient descent loss for a 2-dimensional function. The left graph illustrates a normal descent towards the minimum, while the right graph illustrates a stochastic descent towards the minimum.</figcaption>\n",
    "</figure>\n",
    "\n",
    "##### Setting Fitting Process Parameters\n",
    "Once we defined the `architecture` and the `optimization` parameters of our deep learning solution, we can finally perform a learning process, or `fitting`.\n",
    "There are some parameters we'll new define in order to set our fitting process. The `epochs` defines how many iterations the learning process will perform. The `batch_size` defines the number of samples the algorithm will use in each iteration, and `learning_rate` defines the step size of the `SGD` optimizer. We'll set the first 2 parameters in the following parts, while changing the `learning_rate` only on later parts of this exercise.  \n",
    "\n",
    "\n",
    "Now let's compile our model with the `optimization` parameters using `model.compile()`, and run the learning process using `model.fit()`. The result of the fitting process will be plotted in a `loss vs epochs` graph, showing the training error decreases as the learning process continues.\n",
    "\n",
    "In our case, $x_{train}^m$ (sensor's output) is our input data and $y_{train}^m$ (real fault size) is the output. Number of `epochs` was set to 100, and `batch_size` to 64. Play with those parameters in order to notice their effect on the learning process.\n",
    "\n",
    "*Note 1: In the solution we manage to achieve train loss = 4.9e-3 for 100 epochs*\n",
    "<br>*Note 2: A single run should take about 1 minute*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ee9576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model's architecture\n",
    "model = create_model()\n",
    "\n",
    "# Setting the optimization parameters -MAE loss function and SGD optimzier.\n",
    "model.compile(loss='mean_absolute_error', optimizer='SGD')\n",
    "\n",
    "# Setting learnig rate\n",
    "learning_rate = 0.01     ## Play with this parameter\n",
    "backend.set_value(model.optimizer.learning_rate, learning_rate)\n",
    "\n",
    "# Running learning process\n",
    "history = model.fit(x_train, y_train, epochs=100, batch_size=64, verbose=0)\n",
    "\n",
    "# Printing Results\n",
    "print(\"Train Loss is: \" + \"{:.2e}\".format(history.history['loss'][-1]))\n",
    "\n",
    "# Plotting loss vs epochs\n",
    "plt.semilogy(history.history['loss'], 'b')\n",
    "plt.title(\"Model Loss vs Epochs\")\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c4ad62",
   "metadata": {},
   "source": [
    "`Play with the learning rate parameter in the code segment above and see how it affects the results before processing to the next parts.`\n",
    "\n",
    "### Part 3.4 - Regulation Using Early Stopping\n",
    "As you may have noticed, as we increase the number of `epochs` the `training error` decreases. This result isn't surprising since adding more `epochs` causes the algorithm to learn better the given data. But - when should we stop the algorithm? The number of epochs defines how many learning epochs the algorithm will perform, but we set it kind of arbitrary. In reality we want the algorithm to run just enough time to get a good fitting, but avoiding wasteful runtime.\n",
    "\n",
    "This problem is solved by `regulation`. Applying `regulation` means limiting the algorithm ability to learn the training data \"too much\" so it will be able to generalize well.\n",
    "\n",
    "`Early Stopping` is a method for regulating the number of epochs and it has two purposes:\n",
    "1. **Avoid overfitting** - by monitoring the `validation loss` and stopping the learning process at the point that the `validation loss` stops decreasing and starts increasing. At this point continuing the learning process will harm the learning.\n",
    "2. **Avoid unnecessary runtime** - by monitoring `train loss` and stopping the learning process at the point that the `train loss` stops decreasing and reaches a plateau. At this point continuing the learning process will yield no improvement.\n",
    "\n",
    "Using the dedicated `early stopping` tool provided by `keras` library, we can the number of epochs used by your learning algorithm. In `keras` sequential model, we set the `early stopping` using `callback`. `Callbacks` allow us to perfroms actions at various stages of training (e.g. at the start or end of an epoch, before or after a single batch, etc).\n",
    "<br>Notice that there are several ways to set this tool. We'll set the fitting process to stop after 10 epochs with no improvement for the training loss.\n",
    "\n",
    "`Complete the implementation using callbacks.EarlyStopping(..) in order to stop the process after 10 epochs with no improvement of the training loss`.\n",
    "`For this exercise, a reasonable number of epochs is around 120.`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ca311b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model's architecture and setting the optimization parameters\n",
    "model = create_CNN_model()\n",
    "model.compile(loss='mean_absolute_error', optimizer='SGD')\n",
    "\n",
    "# Set Early Stopping tool to stop after 10 epochs with no improvement.\n",
    "# Complete here:\n",
    "\n",
    "# Running learning process\n",
    "history = model.fit(x_train, y_train, epochs=200, batch_size=64, verbose=0, callbacks=[callback])\n",
    "\n",
    "# Printing Results\n",
    "print(\"Train Loss is: \" + \"{:.2e}\".format(history.history['loss'][-1]))\n",
    "print(\"Number of epochs until stopping: \" + str(np.size(history.history['loss'])))\n",
    "\n",
    "# Plotting loss vs epochs\n",
    "plt.semilogy(history.history['loss'], 'r')\n",
    "plt.title(\"Model Loss vs Epochs\")\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6dad05",
   "metadata": {},
   "source": [
    "### Part 3.5 - Change Learning Rate\n",
    "An important parameter of SGD algorithm is the `learning_rate`, which represents the descent step size of the algorithm.\n",
    "<br>At larger `learning_rate`, the movement is rougher and the chances to miss the local minimum increases, resulting in higher loss.\n",
    "<br>At smaller `learning_rate`, the descent is slower and more epochs are needed to find time local minimum, resulting in slower learning. This effect is illustrated in **Figure 7**.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"img/sgd_rate.jpg\" alt=\"bla\">\n",
    "  <figcaption><b>Figure 7</b> - SGD for different learning rates. The left graph illustrates using a too small learning rate - resulting is slow advancement. The right graph illustrates using a too large learning rate, resulting is high error. The center graph illustrates using an appropriate learning rate, resulting in a reasonable runtime and error.</figcaption>\n",
    "</figure>\n",
    "\n",
    "<br>Play with `learning_rate` parameter of the SGD algorithm and notice its effect. Plot a graph of `Loss vs Epochs` that compares between several `learning_rate` settings.\n",
    "\n",
    "<br>*Note 1: the default `learning_rate` is 0.01.*\n",
    "<br>*Note 2: This run takes several minutes. In order to decrease calculation time we'll only part of the data.*\n",
    "<br>*Note 3: For this part we split the `training set` to `training set` and `validation set` in order to see the affect of different learning rate on the validation set too. The split is achieved by using the `validation_split` parameter as an input for the model.fit() function. `validation_split=0.3` means that 30% of the *training set* will be used for *validation** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056d1da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plot figure\n",
    "fig, ax = plt.subplots(1,2, figsize=(8, 4), sharey='all')\n",
    "\n",
    "# Test some learning rates\n",
    "for rate in [0.100, 0.050, 0.010, 0.001, 0.0001, 0.00001]:\n",
    "    # Reconstructing the architecture of the model\n",
    "    backend.clear_session()\n",
    "    model = create_model()\n",
    "    model.compile(loss='mean_absolute_error', optimizer='SGD')\n",
    "    \n",
    "    # Setting learning rate\n",
    "    backend.set_value(model.optimizer.learning_rate, rate)\n",
    "    print(\"Fitting at learning rate: \" + \"{:.2e}\".format(rate))\n",
    "    \n",
    "    # Running learning process \n",
    "    history = model.fit(x_train, y_train, validation_split=0.3, epochs=150,\n",
    "                            batch_size=64, verbose=0)\n",
    "    \n",
    "    print(\"\\tTrain Loss is: \" + \"{:.2e}\".format(history.history['loss'][-1]) + \n",
    "      \"\\n\\tValidation Loss is: \" + \"{:.2e}\".format(history.history['val_loss'][-1]) + \"\\n\")\n",
    "    # Plot results\n",
    "    ax[0].semilogy(history.history['loss'])\n",
    "    ax[1].semilogy(history.history['val_loss'])\n",
    "\n",
    "ax[0].set_title(\"Training Loss\")\n",
    "ax[1].set_title(\"Validation Loss\")\n",
    "ax[0].set_ylabel('loss')\n",
    "ax[1].set_ylabel('loss')\n",
    "ax[0].set_xlabel('epochs')\n",
    "ax[1].set_xlabel('epochs')\n",
    "plt.legend(['0.100', '0.050', '0.010', '0.001', '0.0001', '0.00001'], loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6065321",
   "metadata": {},
   "source": [
    "A large `learning_rate` (0.1 or 0.05 in this example) prevents the algorithm from finding the minimum.\n",
    "<br>As the `learning_rate` gets smaller, the process becomes slower. For `learning_rate` of 0.01 or 0.001 we observe a steep slope toward the minimum, and a more moderate slope for `learning_rate` of 1e-4. On the other side, we can observe that `learning_rate` of 1e-4 results in a smaller error.\n",
    "<br>The `learning_rate` of 1e-5 is an extreme example of a too slow rate. In 150 epochs, as can be seen from the graph, the algorithm almost didn't minimize the loss function. \n",
    "\n",
    "As can be observer from the `validation loss` graph, it behaves the same as the `training loss`, so there's no overfit.\n",
    "\n",
    "### Part 3.6 - Optimizing Results by Changing Learning Rates\n",
    "\n",
    "Now try to optimize the results by reducing the `learning_rate` every time your result stops improving.\n",
    "<br> The code below implements a learning process that changes its learning rate every after a certain number of epoch passes, or when `early stopping` stops the learning. The `learning_rates` array contains the learning rates that will be used in the learning process. \n",
    "<br>`Set this variable with the learning rates in a decreasing order, and try to achieve loss as small as possible`.\n",
    "\n",
    "*Note: In the solution we manage to achieve train loss = 5.18e-4 and validation loss = 5.55e-4*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1118ac59-4a95-4c19-afc8-9e6fd2fa1ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstructing the architecture of the model\n",
    "backend.clear_session()\n",
    "model = create_model()\n",
    "model.compile(loss='mean_absolute_error', optimizer='SGD')\n",
    "\n",
    "# Initializing looses variables\n",
    "loss = []\n",
    "val_loss = []\n",
    "\n",
    "learning_rates = []# Fill this array in a decreasing order of learning rates.\n",
    "# Fitting with changing learning rates\n",
    "for rate in learning_rates:\n",
    "    \n",
    "    # Setting learning rate\n",
    "    backend.set_value(model.optimizer.learning_rate, rate)\n",
    "    callback = callbacks.EarlyStopping(monitor='loss', patience=6)\n",
    "    print(\"Fitting at learning rate: \" + \"{:.2e}\".format(rate))\n",
    "    \n",
    "    # Running learning process\n",
    "    history = model.fit(x_train, y_train, validation_split=0.3, epochs=60, batch_size=64,\n",
    "                        verbose=0, callbacks=[callback])\n",
    "    \n",
    "    # Updating loss variables\n",
    "    loss = np.concatenate([loss, history.history['loss']])\n",
    "    val_loss = np.concatenate([val_loss, history.history['val_loss']])\n",
    "\n",
    "# Print results\n",
    "print(\"Train Loss is: \" + \"{:.2e}\".format(loss[-1]) + \n",
    "      \"\\nValidation Loss is: \" + \"{:.2e}\".format(val_loss[-1]))\n",
    "\n",
    "# Plot results\n",
    "plt.semilogy(loss, 'r')\n",
    "plt.semilogy(val_loss, 'b')\n",
    "plt.title(\"Model Loss\")\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e61b7c5",
   "metadata": {},
   "source": [
    "Decreasing the learning rate during learning process improves the solution. that's because when the algorithm approaches the local minimum, smaller steps are required to minimize the loss function (see **Figure 8**). Thats the reason decreasing learning rate helps us get better result. We start with a large learning rate in order to make the process faster, and decrease the rate as we get closer to the minimum.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"img/learning_rate_change.png\" alt=\"bla\" style=\"width: 60%\">\n",
    "  <figcaption><b>Figure 8</b> - A approaching the local minimum, decreasing the learning rate should help us find a better minimum.</figcaption>\n",
    "</figure>\n",
    "\n",
    "## Summary\n",
    "This exercise presents the basics of the `deep learning`. In the exercise we've been through:\n",
    "1. The fundamental building blocks of `deep learning` are `architecture`, `optimization` and `regulation` (see **Figure 9**). We implemented those building blocks using `keras` library.\n",
    "2. The benefits of using `deep learning` over `classic ML`. The main benefit is that the engineer doesn't have to understand the data well and doesn't need to define the features. The `deep learning` method extracts the feature itself and the engineer responsibility is to construct the architecture properly and sometimes pre-process the data. Additionally, it can be observed that runtime complexity can also be reduced using `deep learning`.\n",
    "3. Finally, we performed a more advanced optimization by changing the learning rate during the learning process, and achieved even better results.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"img/deep_learning_blocks.png\" alt=\"bla\" style=\"width: 60%\">\n",
    "  <figcaption><b>Figure 9</b> - The building blocks of deep learning.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44960950",
   "metadata": {},
   "source": [
    "Important notes:\n",
    "1. translation invariance\n",
    "2. fully connected vs not (in terms of complexity)\n",
    "3. convolution layer, kernels, max pooling\n",
    "4. comparison for different M\n",
    "5. filter channels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
