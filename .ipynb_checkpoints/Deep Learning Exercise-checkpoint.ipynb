{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "549d94c9",
   "metadata": {},
   "source": [
    "# Deep Learning Intro Exercise\n",
    "\n",
    "In this exercise you'll learn the basics of deep learning and the power of using its methods.\n",
    "The exercise contains four parts:\n",
    "1. Introducing a problem from the field of prognostic health monitoring (PHM) by vibration signals\n",
    "2. Solving the problem using classic machine learning (ML)\n",
    "3. Solving the problem using deep learning\n",
    "4. Optimizing the results\n",
    "\n",
    "*Created by Or Zamir*\n",
    "<br>*Inspected and directed by Omri Matania*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15018dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting HTML styling was successful.\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "import modules.css_settings as css\n",
    "HTML(css.get_settings())\n",
    "print(\"Setting HTML styling was successful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfc2863f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing libraries and dataset was successful.\n"
     ]
    }
   ],
   "source": [
    "## Imports\n",
    "import modules.sample_generator as generator\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import backend\n",
    "from keras import callbacks\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsRegressor as knn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score\n",
    "from scipy.stats import kurtosis\n",
    "import tensorflow as tf\n",
    "\n",
    "## Fetching Dataset\n",
    "x_train, y_train = generator.get_dataset()\n",
    "sample_size = 1024\n",
    "\n",
    "print(\"Importing libraries and dataset was successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f2c795",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "I this part we will introduce the problem and create the data samples.\n",
    "\n",
    "### Part 1.1 - Introducing the Problem\n",
    "You are a very talented engineer in a company that designs and manufactures prognostic health monitoring (PHM) products.\n",
    "In short, those products monitor the vibration of mechanical systems in order to sense a fault of their mechanical parts.\n",
    "The product you are working on is trying to sense a fault in tooth-wheel systems, as illustrated in **figure 1**.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"img/tooth_wheel.png\" alt=\"bla\" width=300 height=300>\n",
    "  <figcaption><b>Figure 1</b> - Two tooth-wheel machine.</figcaption>\n",
    "</figure>\n",
    "\n",
    "When the system has no flaws and the tooth-wheels have no faults, the sensor senses a pulsed signal, with a number of pulses as the number of the teeth.\n",
    "\n",
    "A system fault can be a hole in one of the teeth, as illustrated in **figure 2**.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"img/tooth_defect.png\" alt=\"bla\" width=300 height=300>\n",
    "  <figcaption><b>Figure 2</b> - Two tooth-wheel machine with a fault.</figcaption>\n",
    "</figure>\n",
    "\n",
    "This type of fault is called `full tooth face fault`.\n",
    "\n",
    "For the purpose of this exercise, the fault is modeled by an *additional pulse* in the sensor's output, with *width and height proportional to the fault's* width and depth (let's assume they are correlated, so the width and height of the pulse are proportional to each other).\n",
    "\n",
    "`The code segment below generates a signal sensed by the sensor.\n",
    "Play with the 3 parameters (number of teeth, fault size and noise size) to see how it affects the signal.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "869f991d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set Signal's Parameters\n",
    "number_of_teeth = 15         # Number of teeth of the tooth-wheel, a reasonable range is 5 to 35.\n",
    "fault_size = 0.5            # fault size, ranges from 0 to 1. Other values will return an error.\n",
    "noise_size = 0.3             # The size of the noise, a reasonable range is 0 to 0.5.\n",
    "\n",
    "# Generate signal\n",
    "signal = generator.generate(number_of_teeth, fault_size, noise_size)\n",
    "\n",
    "# Plot\n",
    "plt.plot(signal)\n",
    "plt.title(\"Signal With \" + str(number_of_teeth) + \" Teeth and Fault Size = \" + str(fault_size))\n",
    "plt.xlabel(\"Sample [N]\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51746e5",
   "metadata": {},
   "source": [
    "### Part 1.2 - Defining the Goal\n",
    "Now we will define the goal of our learning machine.\n",
    "The sensor, as mentioned, provides data about the fault in the form of an additional pulse to the signal, proportional to the fault size.\n",
    "\n",
    "```Our goal is to estimate the fault size based on the sensor's measurements.```\n",
    "\n",
    "Our data contains M training samples, when **$x_{train}^{m}$** is the output signal of the sensor and **$y_{train}^{m}$** is the real fault size of the signal.\n",
    "\n",
    "In `machine learning` problems, usually two sets of data are provided - train set and test set. The train set is used to train the learning algorithm, and it's the input of the algorithm. Once the algorithm provides us a solution, the test set is used to test how accurately the algorithm predicts unseen data. For this kind of problems, the train set is usually split into 2 subsets - train and validation. The validation set acts as a regulator to the learning algorithm, as we'll observe later. Illustration of the three data sets is illustrated in **figure 3**.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"img/test_train_validation.png\" alt=\"bla\" width=350 height=350>\n",
    "  <figcaption><b>Figure 3</b> - Test, train and validation sets illustrated.</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03e9deb",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Part 2 - Classic Machine Learning Solution\n",
    "\n",
    "Let's solve this problem with a classic ML approach, using `K-nearest-neighbors` (KNN) algorithm.\n",
    "This algorithm works as follows:\n",
    "1. Define `features` which are correlated with the `fault size`.\n",
    "2. Pre-process the data by extracting its `features` and use it as the algorithm's input.\n",
    "3. Find the K nearest neighbors of $x_{train}^{m}$ samples to your $x_{test}$ input, based on the `features`.\n",
    "4. Calculate the average over the K $y_{train}^{m}$ samples of the nearest  K $x_{train}^{m}$\n",
    "5. The average over $y_{train}^{m}$ samples should estimate the `fault size`.\n",
    "\n",
    "*Note: the K parameter of KNN we should be set by using regulation. This topic will be reviewed in the deep learning section. For the purpose of this exercise we'll use K=10.*\n",
    "\n",
    "The KNN algorithm is illustrated in **Figure 3**. The illustration shows three clusters of samples, while each color represents a different classification target. The colored samples are the training set, and the black sample is a new sample we're trying to predict its classification. The classification of the new sample is determined by the nearest four neighbors (K=4). In this example, most of the neighbors have blue classification, so this sample will be classified blue.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"img/knn.png\" alt=\"bla\" style=\"width: 50%\">\n",
    "  <figcaption><b>Figure 3</b> - KNN illustration with three classification targets.</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "### Part 2.1 - Defining Features\n",
    "The first step is to define the features. In order to achieve good enough results, we'll use several features - lets say four.\n",
    "<br>By looking at the signal and playing with the parameters, we can suggest several features that may be correlated with the `fault size`:\n",
    "1. Signal's `Root Mean Square` (`RMS`)\n",
    "2. Signal's `max value`\n",
    "3. Signal's `kurtosis`\n",
    "4. Signal's `Standard Deviation` (`STD`)\n",
    "\n",
    "```Of course, there are more properties that can be correlated with the fault size - try to think of additional features.```\n",
    "\n",
    "\n",
    "### Part 2.2 - Testing the Features\n",
    "The next step is testing the selected `features` in order to proof that they are indeed correlated with `fault size`.\n",
    "\n",
    "```In the next code segment, complete the implementations of the features and run the program, if the feature correlate with fault size, you'll observe a visual correlation between each feature and the fault size.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d676cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Max value of a signal\n",
    "def signal_max(x):\n",
    "    # Complete implementation here:\n",
    "\n",
    "    return max_val\n",
    "\n",
    "# Calculating RMS of a signal\n",
    "def signal_rms(x):\n",
    "    # Complete implementation here:\n",
    "\n",
    "    return rms\n",
    "\n",
    "# Calculating kurtosis of a signal \n",
    "def signal_kurt(x):\n",
    "    # Complete implementation here:\n",
    "    \n",
    "    return kurt\n",
    "\n",
    "# Calculating SDT of a signal\n",
    "def signal_std(x):\n",
    "    # Complete implementation here:\n",
    "    \n",
    "    return std\n",
    "\n",
    "## Create feature functinos array - if you add any function, add it to this array\n",
    "features_calc = [signal_max,\n",
    "                 signal_rms,\n",
    "                 signal_kurt,\n",
    "                 signal_std]\n",
    "\n",
    "## Testing features\n",
    "# Change the feature_func and the feature_name according to the feature you wish to test.\n",
    "generator.test_feature(feature_func=signal_max, feature_name='Max Value')\n",
    "generator.test_feature(feature_func=signal_rms, feature_name='RMS')\n",
    "generator.test_feature(feature_func=signal_kurt, feature_name='Kurtosis')\n",
    "generator.test_feature(feature_func=signal_std, feature_name='STD')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3149bd",
   "metadata": {},
   "source": [
    "### Part 2.3 - Pre-processing the Data\n",
    "In the previous parts we defined and tested our `features`.\n",
    "\n",
    "Now we'll pre-process the data into a feature matrix that represents our samples.\n",
    "First we split the training dataset into 2 subsets - train and test. Then we calculate the feature matrix that represents our input data.\n",
    "\n",
    "#### Splitting the Dataset\n",
    "In this case we have only a train set, so we will splits it into two sets - train and test. In this part we won't use regulation (this process is used to optimize the K parameter), so we won't create a validation set.\n",
    "\n",
    "#### Normilizaing Features \n",
    "As you may have noticed, every feature ranges at a different range (watch the x-axis of the plots above). Assuming no feature is a better predictor than other, we want all features to have an even weight in the learning process. The way to achieve it is by normalizing the features. Normalizing is performed by subtracting each feature by its mean value, and then dividing by its standard deviation, as follows:\n",
    "\n",
    "<font size=\"3\">$$normilized\\_feature_m^i = \\frac{feature_m^i - E[feature_m]}{\\sigma[feature_m]}$$</font>\n",
    "<br><center><b>Equation 1</b> -  Normalization of feature element $feature_m^i$ of feature vector $feature_m$ </center>\n",
    "\n",
    "The normalization process should be applied on both the train set and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c469c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train set into 2 subsets\n",
    "x_train_knn, x_test_knn, y_train_knn, y_test_knn = train_test_split(x_train, y_train, test_size=0.3)\n",
    "\n",
    "# Variable initialization\n",
    "feature_num = np.size(features_calc)\n",
    "dataset_size = generator.get_set_size()\n",
    "train_size = np.size(y_train_knn)\n",
    "test_size = np.size(y_test_knn)\n",
    "train_features = np.zeros([train_size, feature_num])\n",
    "test_features = np.zeros([test_size, feature_num])\n",
    "\n",
    "# Creating a feature matrix for train set. Its dimentions are (train_size)x(feature_num)\n",
    "for m in np.arange(train_size):\n",
    "    for f in np.arange(feature_num):\n",
    "        train_features[m][f] = features_calc[f](x_train_knn[m][:])\n",
    "\n",
    "# Creating a feature matrix for test set. Its dimentions are (train_size)x(feature_num)\n",
    "for m in np.arange(test_size):\n",
    "    for f in np.arange(feature_num):\n",
    "        test_features[m][f] = features_calc[f](x_test_knn[m][:])\n",
    "    \n",
    "# Normalizing features\n",
    "for f in np.arange(feature_num):\n",
    "    test_features[:][f] = (test_features[:][f] - np.nanmean(train_features[:][f])) / np.nanstd(train_features[:][f])\n",
    "    train_features[:][f] = (train_features[:][f] - np.nanmean(train_features[:][f])) / np.nanstd(train_features[:][f])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9468bf",
   "metadata": {},
   "source": [
    "### Part 2.4 - Estimation using KNN\n",
    "In this part we'll try to predict a fault size using KNN. After pre-processing, we no longer care about how the `signal` looks like, we only care about its extracted `features`.\n",
    "<br>Now, let's construct the `KNN` predictor using `sklearn` library.\n",
    "\n",
    "```In order to implement a regression predictor, use the `KNeighborsRegressor` class from this library.```\n",
    "\n",
    "Few settings for this algorithm:\n",
    "1. `n_neighbors` - number of neighbors used for averaging\n",
    "2. `train_features` - the extracted features\n",
    "3. `y_train` - the known fault sizes\n",
    "\n",
    "<br>*Note: In the solution we managed to achieve test loss = 3.56e-4 for K=10*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdad2255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating KNN, use K = 10 \n",
    "# Complete here:\n",
    "\n",
    "# Calc MAE\n",
    "# Complete here:\n",
    "mae = ...\n",
    "print(\"MAE error: \" + \"{:.2e}\".format(mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee256244",
   "metadata": {},
   "source": [
    "\n",
    "## Part 3 - Deep Learning Solution\n",
    "\n",
    "<br>In the previous parts we solved the problem using `ML`. This method suffers from several problems:\n",
    "1. The engineer must understand the data and have enough experience to define and extract `features` correctly.\n",
    "2. There are many problems that people solve easily (such as recognizing an animal in a picture), but find it very hard to define the `features` for these problems (what are the features of an animal in a picture?). The task of definig good features is sometimes beyond human capabilities. This is the main reason that `ML` approach is very limited for many `AI` applications.\n",
    "3. The `features` defined might be good enough for describing the input, but may be less suitable for a specific `ML` algorithm.\n",
    "\n",
    "the engineers must extract `features` from the dataset in order to train the algorithm. This task can be difficult as you experienced just now, and sometime it's beyond human capabilities.\n",
    "\n",
    "In this part we will solve this problem using a `deep learning` approach.\n",
    "<br> The `deep learning` approach tries to learn the `features` by itself, which makes it a very powerful tool.\n",
    "\n",
    "### Part 3.1 - Deep Learning Introduction and `Keras` Library\n",
    "This solution assumes basic knowledge of deep learning principles. I highly recommend the three-chapter YouTube series made by 3Blue1Brown as a source of learning those basic principles:\n",
    "<br>[Chapter 1 - Neuron Network](https://www.youtube.com/watch?v=aircAruvnKk&t=462s&ab_channel=3Blue1Brown)\n",
    "<br>[Chapter 2 - Learning with Gradient Descent](https://www.youtube.com/watch?v=IHZwWFHWa-w&ab_channel=3Blue1Brown)\n",
    "<br>[Chapter 3 - Backpropagation](https://www.youtube.com/watch?v=Ilg3gGewQ5U&t=1s&ab_channel=3Blue1Brown)\n",
    "\n",
    "For this part we'll use the `keras` library. For further information about this library, please refer to its documentation site.\n",
    "\n",
    "\n",
    "`Lets define the three building blocks of deep learning:`\n",
    "1. **Architecture** - defining the structure of the deep learning network\n",
    "2. **Optimization** - defining the learning algorithm of the deep learning network, aka minimizing the loSs function.\n",
    "3. **Regulation** - defning the the methods we apply in order to avoid overfitting.\n",
    "\n",
    "Each of the following parts will focus on each of the building blocks, along with the solution for the presented problem.\n",
    "\n",
    "### Part 3.2 - Architecture\n",
    "The deep learning architecture represents our prior knowledge of the problem, called `bias`. The Architecture is defined by its `layers number`, `layers size`, `layers type` and the `activation` function of each layer. We'll construct a 3-layer architecture as illustrated in **figure 4**.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"img/architecture.png\" alt=\"bla\" style=\"width: 50%\">\n",
    "  <figcaption><b>Figure 4</b> - Illustration of the 3-layer architecture for this exercise. <br>The properties of each layer (size, type and activation) are shown above each layer.</figcaption>\n",
    "</figure>\n",
    "\n",
    "##### Layers\n",
    "We'll construct a simple architecture with 3 layers:\n",
    "1. The first layer represents the `input layer` (sensor's output), and it has the size of the input (`sample_size`=1024). No activation is applied on the first layer, since it represents the data.\n",
    "2. The second layer is a `hidden layer` and it has a size of 64 (arbitrary size). We will apply a `ReLu activation` for this layer.\n",
    "3. The third layer represents the `output layer` (the fault size), and it's size is 1 (a scalar). We will apply a `sigmoid activation` for this layer.\n",
    "*All layers but the input are of type `dense`, means they are fully connected to their preceding layer. The input layer has no preceding layer, so there's no point in defining its type.*\n",
    "\n",
    "`Activation` functions maps the input of each node in the layer into a value of a specific range (commonly [0 to 1] or [-1 to 1]) that represents how `activated` the node is. As nodes represents feature elements, the `activation` of the node how strong the learning process recognizes this element in the data. **Figure 5** shows three common `activation` functions and their formulas, two of them are used in this exercise.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"img/activations.png\" alt=\"bla\" style=\"width: 80%\">\n",
    "  <figcaption><b>Figure 5</b> - Three common activation functions. From left to right - sigmoid activation, tanh activation and rectified linear unit (ReLu) activation.</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "##### Kernel initializers\n",
    "Defines the initial distribution of the weight vector. We will set this setting to `normal` distribution.\n",
    "\n",
    "`The following code segment implements our model's architecture. Any code line is preceded with a comment explaining the implementation.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4523e14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    # Creating a sequential model - a simple way to construct a model. For constructing more complex models \n",
    "    # with additional flexibility in design, one can use a Functional model.\n",
    "    model = Sequential()\n",
    "\n",
    "    # Adding the input layer with a dimension of the sample_size (1024)\n",
    "    # The input layer doesnt have any more properties.\n",
    "    model.add(keras.layers.InputLayer(input_shape=(sample_size,)))\n",
    "\n",
    "    # Adding a hidden layer with a dimension of the 64 (an arbitrary value)\n",
    "    # The type of layer is Dense, its activation is ReLu and it's weights initialized with normal distribution.\n",
    "    model.add(Dense(64, kernel_initializer='normal', activation='relu', name='hidden_layer'))\n",
    "    \n",
    "    # Adding the output later with the dimension of the output (a scalar)\n",
    "    # The type of layer is Dense, its activation is sigmoid and it's weights initialized with normal distribution.\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid', name='output_layer'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6adc3cc",
   "metadata": {},
   "source": [
    "Finally, lets construct the model and print its architecture using `model.summary()`.  \n",
    "<br>*Note that the input layer doesnt appear in the model summery, it starts only from the second layer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d20284",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42026d8",
   "metadata": {},
   "source": [
    "### Part 3.3 - Optimization\n",
    "The optimization defines the algorithm which solves the learning problem. It includes the `loss function` and the `optimizer`. \n",
    "<br>The `loss function` is the error which the algorithm aims to reduce to minimum. We'll set this parameter to `Mean Absolute Error`, or `MAE` in short. <br>The `optimizer` is the algorithm we use in order to minimize the `loss function`. For this type of problem, we'll use the numerical optimizer called `Stochastic Gradient Descent`, or `SGD` in short.\n",
    "\n",
    "#### Mean Absolute Error\n",
    "The `mean absolute error` is defined by following **Equation 2**:\n",
    "<font size=\"3\">$$MAE=\\frac{\\sum_{i=0}^{N-1}{|predicted_i-true_i|}}{N} $$</font>\n",
    "<br><center><b>Equation 2</b> -  Mean absolute error.</center>\n",
    "\n",
    "#### Stochastic Gradient Descent\n",
    "`Gradient descent` is an iterative optimization algorithm for finding a local minimum of a differential function. In our case, we try to minimize the loss function (`MAE`), and the controlled variables are the weights that transforms the input into the output through the networks layers. The algorithm calculates the `gradient` of the function - a vector of partial derivatives for each variable. Then the algorithm calculated the new value of the function by subtracting the gradient from the previous value.\n",
    "The descent rate is controlled by a design parameter $\\alpha$ as can be seen in **Equation 3**. In `kears` library this design parameter is configured by the `learning_rate` parameter, as you'll experience later.\n",
    "\n",
    "\n",
    "<font size=\"3\">$$Loss_m = loss_{m-1}-\\alpha ∇F(Loss_{m-1}) $$</font>\n",
    "<br><center><b>Equation 3</b> -  Calculation of the $m$-step of the gradient descent algorithm for the loss output by subtracting the gradient of the previous output $loss_{m-1}$ from its value. The gradient is multiplied by a step_size parameter $\\alpha$ before subtraction to control the rate and the accuracy of the descent.</center>\n",
    "\n",
    "<br><br>\n",
    "`Stochastic gradient descent` is a version of `gradient descent` that uses only a subset of the samples (called `mini-batch`) in order to calculate the gradient. The advantage is obvious - calculation of less samples *reduces runtime*. The tradeoff is that the descent is less accurate for each step, and therefore can yield worse results. The difference in the descent movement is illustrated in **Figure 6**. \n",
    "\n",
    "<figure>\n",
    "  <img src=\"img/sgd.png\" alt=\"bla\" style=\"width: 80%\">\n",
    "  <figcaption><b>Figure 6</b> - Gradient descent loss for a 2-dimensional function. The left graph illustrates a normal descent towards the minimum, while the right graph illustrates a stochastic descent towards the minimum.</figcaption>\n",
    "</figure>\n",
    "\n",
    "##### Setting Fitting Process Parameters\n",
    "Once we defined the `architecture` and the `optimization` parameters of our deep learning solution, we can finally perform a learning process, or `fitting`.\n",
    "There are some parameters we'll new define in order to set our fitting process. The `epochs` defines how many iterations the learning process will perform. The `batch_size` defines the number of samples the algorithm will use in each iteration, and `learning_rate` defines the step size of the `SGD` optimizer. We'll set the first 2 parameters in the following parts, while changing the `learning_rate` only on later parts of this exercise.  \n",
    "\n",
    "\n",
    "Now let's compile our model with the `optimization` parameters using `model.compile()`, and run the learning process using `model.fit()`. The result of the fitting process will be plotted in a `loss vs epochs` graph, showing the training error decreases as the learning process continues.\n",
    "\n",
    "In our case, $x_{train}^m$ (sensor's output) is our input data and $y_{train}^m$ (real fault size) is the output. Number of `epochs` was set to 100, and `batch_size` to 64. Play with those parameters in order to notice their effect on the learning process.\n",
    "\n",
    "*Note 1: In the solution we manage to achieve train loss = 4.9e-3 for 100 epochs*\n",
    "<br>*Note 2: A single run should take about 1 minute*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ee9576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model's architecture\n",
    "model = create_model()\n",
    "\n",
    "# Setting the optimization parameters -MAE loss function and SGD optimzier.\n",
    "model.compile(loss='mean_absolute_error', optimizer='SGD')\n",
    "\n",
    "# Setting learnig rate\n",
    "learning_rate = 0.01     ## Play with this parameter\n",
    "backend.set_value(model.optimizer.learning_rate, learning_rate)\n",
    "\n",
    "# Running learning process\n",
    "history = model.fit(x_train, y_train, epochs=100, batch_size=64, verbose=0)\n",
    "\n",
    "# Printing Results\n",
    "print(\"Train Loss is: \" + \"{:.2e}\".format(history.history['loss'][-1]))\n",
    "\n",
    "# Plotting loss vs epochs\n",
    "plt.semilogy(history.history['loss'], 'b')\n",
    "plt.title(\"Model Loss vs Epochs\")\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c4ad62",
   "metadata": {},
   "source": [
    "`Play with the learning rate parameter in the code segment above and see how it affects the results before processing to the next parts.`\n",
    "\n",
    "### Part 3.4 - Regulation Using Early Stopping\n",
    "As you may have noticed, as we increase the number of `epochs` the `training error` decreases. This result isn't surprising since adding more `epochs` causes the algorithm to learn better the given data. But - when should we stop the algorithm? The number of epochs defines how many learning epochs the algorithm will perform, but we set it kind of arbitrary. In reality we want the algorithm to run just enough time to get a good fitting, but avoiding wasteful runtime.\n",
    "\n",
    "This problem is solved by `regulation`. Applying `regulation` means limiting the algorithm ability to learn the training data \"too much\" so it will be able to generalize well.\n",
    "\n",
    "`Early Stopping` is a method for regulating the number of epochs and it has two purposes:\n",
    "1. **Avoid overfitting** - by monitoring the `validation loss` and stopping the learning process at the point that the `validation loss` stops decreasing and starts increasing. At this point continuing the learning process will harm the learning.\n",
    "2. **Avoid unnecessary runtime** - by monitoring `train loss` and stopping the learning process at the point that the `train loss` stops decreasing and reaches a plateau. At this point continuing the learning process will yield no improvement.\n",
    "\n",
    "Using the dedicated `early stopping` tool provided by `keras` library, we can the number of epochs used by your learning algorithm. In `keras` sequential model, we set the `early stopping` using `callback`. `Callbacks` allow us to perfroms actions at various stages of training (e.g. at the start or end of an epoch, before or after a single batch, etc).\n",
    "<br>Notice that there are several ways to set this tool. We'll set the fitting process to stop after 10 epochs with no improvement for the training loss.\n",
    "\n",
    "`Complete the implementation using callbacks.EarlyStopping(..) in order to stop the process after 10 epochs with no improvement of the training loss`.\n",
    "`For this exercise, a reasonable number of epochs is around 120.`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ca311b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model's architecture and setting the optimization parameters\n",
    "model = create_model()\n",
    "model.compile(loss='mean_absolute_error', optimizer='SGD')\n",
    "\n",
    "# Set Early Stopping tool to stop after 10 epochs with no improvement.\n",
    "# Complete here:\n",
    "\n",
    "# Running learning process\n",
    "history = model.fit(x_train, y_train, epochs=200, batch_size=64, verbose=0, callbacks=[callback])\n",
    "\n",
    "# Printing Results\n",
    "print(\"Train Loss is: \" + \"{:.2e}\".format(history.history['loss'][-1]))\n",
    "print(\"Number of epochs until stopping: \" + str(np.size(history.history['loss'])))\n",
    "\n",
    "# Plotting loss vs epochs\n",
    "plt.semilogy(history.history['loss'], 'r')\n",
    "plt.title(\"Model Loss vs Epochs\")\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6dad05",
   "metadata": {},
   "source": [
    "### Part 3.5 - Change Learning Rate\n",
    "An important parameter of SGD algorithm is the `learning_rate`, which represents the descent step size of the algorithm.\n",
    "<br>At larger `learning_rate`, the movement is rougher and the chances to miss the local minimum increases, resulting in higher loss.\n",
    "<br>At smaller `learning_rate`, the descent is slower and more epochs are needed to find time local minimum, resulting in slower learning. This effect is illustrated in **Figure 7**.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"img/sgd_rate.jpg\" alt=\"bla\">\n",
    "  <figcaption><b>Figure 7</b> - SGD for different learning rates. The left graph illustrates using a too small learning rate - resulting is slow advancement. The right graph illustrates using a too large learning rate, resulting is high error. The center graph illustrates using an appropriate learning rate, resulting in a reasonable runtime and error.</figcaption>\n",
    "</figure>\n",
    "\n",
    "<br>Play with `learning_rate` parameter of the SGD algorithm and notice its effect. Plot a graph of `Loss vs Epochs` that compares between several `learning_rate` settings.\n",
    "\n",
    "<br>*Note 1: the default `learning_rate` is 0.01.*\n",
    "<br>*Note 2: This run takes several minutes. In order to decrease calculation time we'll only part of the data.*\n",
    "<br>*Note 3: For this part we split the `training set` to `training set` and `validation set` in order to see the affect of different learning rate on the validation set too. The split is achieved by using the `validation_split` parameter as an input for the model.fit() function. `validation_split=0.3` means that 30% of the *training set* will be used for *validation** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056d1da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plot figure\n",
    "fig, ax = plt.subplots(1,2, figsize=(8, 4), sharey='all')\n",
    "\n",
    "# Test some learning rates\n",
    "for rate in [0.100, 0.050, 0.010, 0.001, 0.0001, 0.00001]:\n",
    "    # Reconstructing the architecture of the model\n",
    "    backend.clear_session()\n",
    "    model = create_model()\n",
    "    model.compile(loss='mean_absolute_error', optimizer='SGD')\n",
    "    \n",
    "    # Setting learning rate\n",
    "    backend.set_value(model.optimizer.learning_rate, rate)\n",
    "    print(\"Fitting at learning rate: \" + \"{:.2e}\".format(rate))\n",
    "    \n",
    "    # Running learning process \n",
    "    history = model.fit(x_train, y_train, validation_split=0.3, epochs=150,\n",
    "                            batch_size=64, verbose=0)\n",
    "    \n",
    "    print(\"\\tTrain Loss is: \" + \"{:.2e}\".format(history.history['loss'][-1]) + \n",
    "      \"\\n\\tValidation Loss is: \" + \"{:.2e}\".format(history.history['val_loss'][-1]) + \"\\n\")\n",
    "    # Plot results\n",
    "    ax[0].semilogy(history.history['loss'])\n",
    "    ax[1].semilogy(history.history['val_loss'])\n",
    "\n",
    "ax[0].set_title(\"Training Loss\")\n",
    "ax[1].set_title(\"Validation Loss\")\n",
    "ax[0].set_ylabel('loss')\n",
    "ax[1].set_ylabel('loss')\n",
    "ax[0].set_xlabel('epochs')\n",
    "ax[1].set_xlabel('epochs')\n",
    "plt.legend(['0.100', '0.050', '0.010', '0.001', '0.0001', '0.00001'], loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6065321",
   "metadata": {},
   "source": [
    "A large `learning_rate` (0.1 or 0.05 in this example) prevents the algorithm from finding the minimum.\n",
    "<br>As the `learning_rate` gets smaller, the process becomes slower. For `learning_rate` of 0.01 or 0.001 we observe a steep slope toward the minimum, and a more moderate slope for `learning_rate` of 1e-4. On the other side, we can observe that `learning_rate` of 1e-4 results in a smaller error.\n",
    "<br>The `learning_rate` of 1e-5 is an extreme example of a too slow rate. In 150 epochs, as can be seen from the graph, the algorithm almost didn't minimize the loss function. \n",
    "\n",
    "As can be observer from the `validation loss` graph, it behaves the same as the `training loss`, so there's no overfit.\n",
    "\n",
    "### Part 3.6 - Optimizing Results by Changing Learning Rates\n",
    "\n",
    "Now try to optimize the results by reducing the `learning_rate` every time your result stops improving.\n",
    "<br> The code below implements a learning process that changes its learning rate every after a certain number of epoch passes, or when `early stopping` stops the learning. The `learning_rates` array contains the learning rates that will be used in the learning process. \n",
    "<br>`Set this variable with the learning rates in a decreasing order, and try to achieve loss as small as possible`.\n",
    "\n",
    "*Note: In the solution we manage to achieve train loss = 5.18e-4 and validation loss = 5.55e-4*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1118ac59-4a95-4c19-afc8-9e6fd2fa1ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstructing the architecture of the model\n",
    "backend.clear_session()\n",
    "model = create_model()\n",
    "model.compile(loss='mean_absolute_error', optimizer='SGD')\n",
    "\n",
    "# Initializing looses variables\n",
    "loss = []\n",
    "val_loss = []\n",
    "\n",
    "learning_rates = []# Fill this array in a decreasing order of learning rates.\n",
    "# Fitting with changing learning rates\n",
    "for rate in learning_rates:\n",
    "    \n",
    "    # Setting learning rate\n",
    "    backend.set_value(model.optimizer.learning_rate, rate)\n",
    "    callback = callbacks.EarlyStopping(monitor='loss', patience=6)\n",
    "    print(\"Fitting at learning rate: \" + \"{:.2e}\".format(rate))\n",
    "    \n",
    "    # Running learning process\n",
    "    history = model.fit(x_train, y_train, validation_split=0.3, epochs=60, batch_size=64,\n",
    "                        verbose=0, callbacks=[callback])\n",
    "    \n",
    "    # Updating loss variables\n",
    "    loss = np.concatenate([loss, history.history['loss']])\n",
    "    val_loss = np.concatenate([val_loss, history.history['val_loss']])\n",
    "\n",
    "# Print results\n",
    "print(\"Train Loss is: \" + \"{:.2e}\".format(loss[-1]) + \n",
    "      \"\\nValidation Loss is: \" + \"{:.2e}\".format(val_loss[-1]))\n",
    "\n",
    "# Plot results\n",
    "plt.semilogy(loss, 'r')\n",
    "plt.semilogy(val_loss, 'b')\n",
    "plt.title(\"Model Loss\")\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e61b7c5",
   "metadata": {},
   "source": [
    "Decreasing the learning rate during learning process improves the solution. that's because when the algorithm approaches the local minimum, smaller steps are required to minimize the loss function (see **Figure 8**). Thats the reason decreasing learning rate helps us get better result. We start with a large learning rate in order to make the process faster, and decrease the rate as we get closer to the minimum.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"img/learning_rate_change.png\" alt=\"bla\" style=\"width: 60%\">\n",
    "  <figcaption><b>Figure 8</b> - A approaching the local minimum, decreasing the learning rate should help us find a better minimum.</figcaption>\n",
    "</figure>\n",
    "\n",
    "## Summary\n",
    "This exercise presents the basics of the `deep learning`. In the exercise we've been through:\n",
    "1. The fundamental building blocks of `deep learning` are `architecture`, `optimization` and `regulation` (see **Figure 9**). We implemented those building blocks using `keras` library.\n",
    "2. The benefits of using `deep learning` over `classic ML`. The main benefit is that the engineer doesn't have to understand the data well and doesn't need to define the features. The `deep learning` method extracts the feature itself and the engineer responsibility is to construct the architecture properly and sometimes pre-process the data. Additionally, it can be observed that runtime complexity can also be reduced using `deep learning`.\n",
    "3. Finally, we performed a more advanced optimization by changing the learning rate during the learning process, and achieved even better results.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"img/deep_learning_blocks.png\" alt=\"bla\" style=\"width: 60%\">\n",
    "  <figcaption><b>Figure 9</b> - The building blocks of deep learning.</figcaption>\n",
    "</figure>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
